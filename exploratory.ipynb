{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"The 'Alternative für Deutschland' and their usage of parliamentary speeches on Social Media\"\n",
    "\n",
    "## Information about the research project\n",
    "\n",
    "In this project, which is the Masters Thesis of Moritz Stockmar, the usage of parliamentary speeches by the 'Alternative für Deutschland' (AfD) on Social Media is analyzed. The main objective is to learn which speeches (and parts thereof) are used by the AfD on TikTok and YouTube Shorts, what differentiates them form the population of all of AfD's parliamentary speeches.\n",
    "\n",
    "To achieve this goal the following steps are taken:\n",
    "\n",
    "1. Collecting the data: The speeches of the AfD are collected from the plenary protocolls which can be found on the official website of the Bundestag. The TikTok and YouTube Shorts videos are collected from official social media accounts of the AfD.\n",
    "\n",
    "2. Preprocessing the data: A corpus of AfD speeches during the 20. legislative period of the Bundestag is built. The short videos are transcribed and matched to the corpus entries.\n",
    "\n",
    "3. Analyzing the data: After alligning the uploaded speeches with the official parliamentary protocols, the speeches are analyzed using a variety of methods, such as topic modeling, sentiment analysis, syntactical analysis and more.\n",
    "\n",
    "4. Visualising the data: The results are visualized to be presented in the Masters Thesis.\n",
    "\n",
    "The steps above are also reflected in the structure of this md-document. Refer to the respective sections for more detailled information.\n",
    "The data was (will be) collected by the author and is available on request. The goal is, that this markdown will be self contained and can be used to reproduce the results of the thesis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setting global options and loading the required libraries\n",
    "The following sections load the required libraries and sets some global variables. This sections must be run before any other section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_md-3.8.0/de_core_news_md-3.8.0-py3-none-any.whl (44.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_md')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n@TODO: The following code builds the recquired file structure for the project.\\n'"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import bundestag_api\n",
    "import os\n",
    "import re\n",
    "import whisper\n",
    "import subprocess\n",
    "import ast\n",
    "import unicodedata\n",
    "import difflib\n",
    "\n",
    "from spacy import displacy\n",
    "from time import strftime, localtime\n",
    "\n",
    "# Dowmload the required models\n",
    "\n",
    "# Download and load the spaCy model\n",
    "!python -m spacy download de_core_news_md\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "# @TODO: Trying it with a bigger model\n",
    "\n",
    "\n",
    "# Whisper: The Turbo Model is used. It is 1.5 GB big in storage and uses roughly 6 GB of RAM. \n",
    "# https://huggingface.co/openai/whisper-large-v3-turbo\n",
    "model = whisper.load_model(\"turbo\")\n",
    "# Hugging Face Models\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "@TODO: The following code builds the recquired file structure for the project.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setting global variables\n",
    "BUNDESTAG_API = \"I9FKdCn.hbfefNWCY336dL6x62vfwNKpoN2RZ1gp21\"\n",
    "period_start = '2021-10-01'\n",
    "period_end = '2025-03-24'\n",
    "\n",
    "path_yt_faction = \"data/raw_data/videos/YouTube/AfD-Fraktion Bundestag (Parliamentary Faction)/\"\n",
    "path_yt_party = \"data/raw_data/videos/YouTube/AfD-TV (Party)/\"\n",
    "path_tt = \"data/raw_data/videos/TikTok/\"\n",
    "paths_to_folders = [path_yt_faction, path_yt_party, path_tt]\n",
    "\n",
    "parties = [\"CDU/CSU\", \"SPD\", \"AfD\", \"FDP\", \"BÜNDNIS 90/DIE GRÜNEN\", \"Die Linke\", \"BSW\" \"fraktionslos\"]\n",
    "länder = [\"Baden-Württemberg\", \"Bayern\", \"Berlin\", \"Brandenburg\", \"Bremen\", \"Hamburg\", \"Hessen\", \"Mecklenburg-Vorpommern\", \"Niedersachsen\", \"Nordrhein-Westfalen\", \"Rheinland-Pfalz\", \"Saarland\", \"Sachsen\", \"Sachsen-Anhalt\", \"Schleswig-Holstein\", \"Thüringen\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collecting the Data\n",
    "The plenary protocolls are collected using the [official API](https://dip.bundestag.de/%C3%BCber-dip/hilfe/api) of the German Bundestag. The [python-wrapper](https://github.com/jschibberges/Bundestag-API) by jschibberges is used to do it. The short videos were downloaded via the tool [youtube-dlp](https://github.com/yt-dlp/yt-dlp). This won't be shown here but the video database can be accessed on request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Downloading the speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Collecting the protocols from the Bundestag API from the first session of the 20th legislative period \n",
    "to the date of the election of the 21st legislative period. \n",
    "\n",
    "This codeblock takes some time to run as it collects the data from the Bundestag API. There are 212 protocols to download. \n",
    "Only do this once, as the data is saved to a CSV file for long term storage and the dataframe is pickled for shorter term storage.\n",
    "\n",
    "@TODO: Add the sessions after the election of the 21st legislative period to the data collection\n",
    "\"\"\"\n",
    "bta = bundestag_api.btaConnection(apikey = BUNDESTAG_API)\n",
    "protocols = bta.search_plenaryprotocol(date_start = period_start, date_end = period_end, institution = 'BT', num = 400, fulltext = True)\n",
    "protocols_df  = pd.DataFrame(protocols)\n",
    "\n",
    "# Save the dataframe\n",
    "protocols_df.to_csv(\"data/raw_data/protocols.csv\")\n",
    "protocols_df.to_pickle(\"data/raw_data/protocols.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Downloading the MdB Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download persons from the Bundestag API. It is not completly clear how search_person works, therefore we will download all persons and filter them later.\n",
    "bta = bundestag_api.btaConnection(apikey = BUNDESTAG_API)\n",
    "persons = bta.search_person(updated_since = period_start + \"T00:00:00\", num = 3000)\n",
    "persons_df = pd.DataFrame(persons)\n",
    "\n",
    "# Filter the persons_df for AfD speakers, these can only be MdBs as the AfD does not have any ministers or MdBRs (Memebers of the Bundesrat)\n",
    "# There are a few to many persons which can be explained by people leaving the faction (and now being crossbencher) or leaving parliament during the legislative period\n",
    "filtered_afd_mdbs_df = persons_df[(persons_df['person_roles'].str.contains('AfD', na = False))| \n",
    "                                (persons_df['titel'].str.contains('AfD', na = False))]\n",
    "\n",
    "# Filter the persons_df for non AfD speakers, these can be MdBs, Ministers and MdBRs \n",
    "# There is no need to filter for party other than not being in the AfD\n",
    "filtered_non_afd_speekers_df = persons_df[~((persons_df['person_roles'].str.contains('AfD', na = False))| \n",
    "                                (persons_df['titel'].str.contains('AfD', na = False)))]\n",
    "\n",
    "# Filters everyone out did not speak in the 20th legislative period\n",
    "filtered_non_afd_speekers_df = filtered_non_afd_speekers_df.loc[\n",
    "    # Simple lookup for the wahlperiode == 20 (This applies to everyone speaking for the first time in the 20th legislative period)\n",
    "    (filtered_non_afd_speekers_df['wahlperiode'] == 20) |\n",
    "    # Complicated lookup for everyone else (This applies to everyone who spoke before the 20th legislative period and in the 20th) \n",
    "    (filtered_non_afd_speekers_df['person_roles'].apply(\n",
    "        lambda roles: isinstance(roles, list) and\n",
    "        any(20 in role.get('wahlperiode_nummer', []) for role in roles if isinstance(role, dict))\n",
    "    ))\n",
    "]\n",
    "\n",
    "# Save the dataframe\n",
    "filtered_afd_mdbs_df.to_csv(\"data/raw_data/afd_mdbs.csv\")\n",
    "filtered_afd_mdbs_df.to_pickle(\"data/raw_data/afd_mdbs.pkl\")\n",
    "\n",
    "filtered_non_afd_speekers_df.to_csv(\"data/raw_data/non_afd_speakers.csv\")\n",
    "filtered_non_afd_speekers_df.to_pickle(\"data/raw_data/non_afd_speakers.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the data\n",
    "\n",
    "In the following the preprocessing of the textual and the audio-visual data is performed. \n",
    "\n",
    "1. The textual data (the protocols) is processed into a annotated Corpus consisting of all speeches by AfD MdBs (members of the Bundestag). Therefor firstly all speeches of AfD MdBs have to be extracted (in plain text) from the protocols. The next step is the linguistical preprocessing performed by spaCy to make the data usable for the upcoming syntactical and semantical examinations.\n",
    "\n",
    "2. The audio-visual data (the uploaded short videos) have to be transcribed.\n",
    "\n",
    "3. The transcriptions are matched with their corresponding speeches from the frist step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preprocessing the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Extracting the AfD Speeches from the protocols\n",
    "\n",
    "The following codeblock extracts the AfD Speeches from the protocols and stores them in a single dataframe\n",
    "\n",
    "Attention: This block takes roughly 30s to complete on a M2 Mac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following code block reads the protocols from the CSV file / pickeled file and loads them into a pandas dataframe for further processing\n",
    "if there is no dataframe already created.\n",
    "\"\"\"\n",
    "if os.path.exists(\"data/raw_data/protocols.pkl\"):\n",
    "    protocols_df = pd.read_pickle(\"data/raw_data/protocols.pkl\")\n",
    "elif os.path.exists(\"data/raw_data/protocols.csv\"):\n",
    "    protocols_df = pd.read_csv(\"data/raw_data/protocols.csv\")\n",
    "else:\n",
    "    print(\"No data found. Please run the data collection code block first.\")\n",
    "\n",
    "\"\"\"The afd_mdbs_searchstrings are the strings signaling the start of every AfD Speech in the protocols by being of the form\n",
    "'[o: Titels] [first name] [o: infix] [last name], (AfD):' -> things in brackets are individual, things with o: are optional\n",
    "This searchstring can be used as is to find the start of the speech in the protocol \"\"\"\n",
    "afd_mdbs_searchstrings = filtered_afd_mdbs_df.apply(lambda row: row['titel'].replace(', MdB', '').replace('Dr. ', '').replace(', AfD', ' (AfD)') + \":\", axis = 1).to_list()\n",
    "\n",
    "\"\"\"The non_afd_searchstring is part of the string signaling the start of every non AfD speech in the protocols by being of the form\n",
    "'[o: Titels] [first name] [o: infix] [last name]', The party or affiliation to the federal government or bundesrat is not included, because that would lead to many edge cases\n",
    "This searchsting can NOT be used as is to find the start of the speech in the protocol \"\"\"\n",
    "non_afd_searchstings = filtered_non_afd_speekers_df.apply(lambda row: row['titel'].split(',')[0], axis = 1).to_list()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Helper method to check if a line is the start of an AfD speech. This is done by checking if the line contains any of the searchstrings\n",
    "and if the line does not contain the word \"Frage\" which indicates that the line is a purely written question and not a speech. \n",
    "These are found at the end of protocols if the time was to short to answer each question for the government orally.\n",
    "\"\"\"\n",
    "def is_start_of_AfDSpeech(line, searchstrings) -> bool:\n",
    "    return any(searchstring in line for searchstring in searchstrings) and not \"Frage\" in line\n",
    "\n",
    "\"\"\"\n",
    "Helper method to check if a line is the end of an AfD speech. This is done by checking if the line contains any of the searchstrings\n",
    "and if the line ends with a colon as otherwise references to people would be falsely identified as the end of a speech. Furthermore\n",
    "the line should not start with a bracket as this indicates comments from the audience.\n",
    "\"\"\"\n",
    "def is_end_of_AfDSpeech(line, searchstrings) -> bool:\n",
    "    return any(searchstring in line for searchstring in searchstrings) and line[-1] == ':' and line[0] != '['\n",
    "\n",
    "\"\"\"\n",
    "Helper method to finde the session (as in X. Sitzung der 20. Wahlperiode) and the date of the session in the protocol.\n",
    "@Input: protocol: The protocol as a list of strings (lines). You only need the first 5\n",
    "@Output: A tuple of the form (session, session_date)\n",
    "\"\"\"\n",
    "\"\"\" def find_session_and_date(protocol_lines) -> tuple:\n",
    "    session = protocol_lines[0].split('/')[-1] # The session is after the last '/' in the first line\n",
    "    session_date = protocol_lines[4].split(',')[-1].replace(' den ', '') # The date is after the last ', den ' in the fifth line\n",
    "    return (session, session_date) \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Helper method to find the start of a agenda item \n",
    "\"\"\"\n",
    "def is_start_of_agenda_item(line) -> bool:\n",
    "    line = line.lower()\n",
    "    # The agenda item is always after something like \"ich rufe\" or \"wir kommen\" and contains either \"Tagesordnungspunkt\" or \"Zusatzpunkt\"\n",
    "    # Logical structure (expression for calling new agenda item) and (numercial expression for agenda item)\n",
    "    return (\"rufe\" in line or \"kommen\" in line or \"komme\" in line or bool(re.search('setzen (.*) fort', line))) and (bool(re.search('(tagesordnungspunkte?|zusatzpunkte?) [0-9]?[0-9]?', line)))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Main method to extract the speeches from a protocol. \n",
    "@Input: protocol: The protocol as a string\n",
    "        afd_mdbs_searchstrings: The searchstrings to identify the start of an AfD speech\n",
    "        non_afd_searchstrings: The searchstrings to identify the end of an AfD speech\n",
    "@Output: A Dataframe of the form {'speaker': speaker, 'text': text, 'session': session, 'session_date': session_date, 'agenda_item': agenda_item}\n",
    "\n",
    "The method works by iterating over the normalized lines of the protocol and checking if a line is the start of an AfD speech. If it is, the method\n",
    "starts to collect the text of the speech until it finds the end of the speech. \n",
    "\n",
    "Afterwards a first structural cleanup is done. This means that speeches that are split due to interventions by the prisiding officer (espacially because of time) are concatenated.\n",
    "\n",
    "@TODO: agenda_items is not working correctly yet. It is sometimes empty because of the hard coded way of finding the agenda item.\n",
    "\"\"\"\n",
    "def extract_speeches_from_protocol(protocol, session, date, afd_mdbs_searchstrings, non_afd_searchstings) -> pd.DataFrame:\n",
    "    protocol_lines = protocol.split('\\n')\n",
    "\n",
    "    # First cleanup: Text normalization\n",
    "    normailzed_protocol_lines = [unicodedata.normalize(\"NFKC\", line) for line in protocol_lines]\n",
    "\n",
    "    speeches = []\n",
    "    speech_text = \"\"\n",
    "    in_speech = False\n",
    "    speaker = \"\"\n",
    "    agenda_item = \"\"\n",
    "\n",
    "    for line in normailzed_protocol_lines:\n",
    "        if line == \"\": # Skip empty lines alltogether\n",
    "            continue\n",
    "        if is_start_of_agenda_item(line):\n",
    "            i = 1\n",
    "            while(normailzed_protocol_lines[normailzed_protocol_lines.index(line) + i] == \"\"): #Skip empty lines between the saying \"Ich rufe...\" and the name of the\n",
    "                i += 1\n",
    "            agenda_item = normailzed_protocol_lines[normailzed_protocol_lines.index(line) + i]\n",
    "        if in_speech and is_end_of_AfDSpeech(line, non_afd_searchstings):\n",
    "            speeches.append({\n",
    "                'speaker': speaker, \n",
    "                'text': speech_text, \n",
    "                'session': session, \n",
    "                'session_date': date, \n",
    "                'agenda_item': agenda_item}) # Append the speech (dict) to the list of speeches\n",
    "            speech_text, speaker = \"\", \"\" # Reset the variables\n",
    "            in_speech = False\n",
    "        if in_speech:\n",
    "            speech_text += line\n",
    "        if not in_speech and is_start_of_AfDSpeech(line, afd_mdbs_searchstrings):\n",
    "            in_speech = True\n",
    "            speaker = line.replace(\" (AfD):\", \"\")\n",
    "\n",
    "    # Structural Cleanup: Concat speeches that are split due to interventions by the speaker due to time\n",
    "    cleaned_speeches = []\n",
    "    for speech in speeches:\n",
    "        try:\n",
    "            if speech['text'].startswith('–'): # Sometimes '-' are used to indicate that a speech is split\n",
    "                cleaned_speeches[-1]['text'] += speech['text']\n",
    "                cleaned_speeches[-1]['text'].replace('––', ' ')\n",
    "            elif len(speech['text'].split(\" \")) < 40: # This is a heuristic to determine if the speech is split because of an intervention because of time, \n",
    "                #as the rest of a speech after an intervention is usually shorter than 40 words and a new speech certainly would be longer\n",
    "                cleaned_speeches[-1]['text'] += \" \" + speech['text']\n",
    "            else:\n",
    "                cleaned_speeches.append(speech)\n",
    "        except:\n",
    "            cleaned_speeches.append(speech)\n",
    "    return pd.DataFrame(cleaned_speeches)\n",
    "\n",
    "#Function call and data storage, takes roughly 30 seconds to run\n",
    "afd_speeches = pd.DataFrame()\n",
    "for _, row in protocols_df.iterrows():\n",
    "    # Get list of dict\n",
    "    extracted_speeches = extract_speeches_from_protocol(\n",
    "        protocol = row['text'], \n",
    "        session = row['dokumentnummer'].split('/')[1],\n",
    "        date = row['datum'], \n",
    "        afd_mdbs_searchstrings = afd_mdbs_searchstrings, \n",
    "        non_afd_searchstings = non_afd_searchstings)\n",
    "\n",
    "    # Convert list of dict to dataframe\n",
    "    afd_speeches = pd.concat([afd_speeches, extracted_speeches], ignore_index = True)\n",
    "\n",
    "afd_speeches.to_csv(\"data/preprocessed_data/afd_speeches.csv\")\n",
    "afd_speeches.to_pickle(\"data/preprocessed_data/afd_speeches.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Bulding the annotated corpus of AfD speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following code block reads the speeches of the AfD MdBs from the CSV file / pickeled file and loads them into a pandas dataframe for further processing\n",
    "if there is no dataframe already created.\n",
    "\"\"\"\n",
    "if  afd_speeches.empty:\n",
    "    if os.path.exists(\"data/preprocessed_data/afd_speeches.pkl\"):\n",
    "        afd_speeches = pd.read_pickle(\"data/preprocessed_data/afd_speeches.pkl\")\n",
    "    elif os.path.exists(\"data/preprocessed_data/afd_speeches.csv\"):\n",
    "        afd_speeches = pd.read_csv(\"data/preprocessed_data/afd_speeches.csv\")\n",
    "    else: \n",
    "        print(\"No data found. Please run the data collection code block first.\")\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The followng code block is used to clean up the speeches further.\n",
    "Cleand up things:\n",
    " - Interjections by the listeners\n",
    "\"\"\"\n",
    "\n",
    "def further_cleanup(speeches: pd.DataFrame) -> pd.DataFrame:\n",
    "    cleaned_speeches = speeches.copy()\n",
    "    # Remove interjections by the listeners, these are idicated by brackets in the protocol. As far as I can tell,\n",
    "    # this is the only place where brackets are used in a speech.\n",
    "    cleaned_speeches['text'] = cleaned_speeches['text'].apply(lambda x: re.sub(r'\\(.*?\\)', '', x))\n",
    "    return cleaned_speeches\n",
    "\n",
    "afd_speeches_cleaned = further_cleanup(afd_speeches)\n",
    "\n",
    "\"\"\"\n",
    "Some helper methods to simplfy the .apply statements in the following code block\n",
    "\"\"\"\n",
    "def get_token(doc):\n",
    "    return [token.text for token in doc]\n",
    "def get_lemma(doc):\n",
    "    return [token.lemma_ for token in doc]\n",
    "def get_pos(doc):\n",
    "    return [(token.pos_, token.tag_) for token in doc]\n",
    "\n",
    "# Preprocessing the speeches with the spaCy pipeline (tokenization, lemmatization, POS tagging, etc. pp.)\n",
    "# As you can guess, this takes some time to run (with an Apple M2 the pipeline itself (building the doc) took 1:45 minutes)\n",
    "\n",
    "def spacy_pipeline(cleaned_speeches: pd.DataFrame) -> pd.DataFrame:\n",
    "    if not 'doc' in cleaned_speeches.columns:\n",
    "       cleaned_speeches['doc'] = cleaned_speeches['text'].apply(nlp)\n",
    "    print(\"Pipeline finished.\")\n",
    "    if not 'tokens' in cleaned_speeches.columns:\n",
    "        cleaned_speeches['tokens'] = cleaned_speeches['doc'].apply(get_token)\n",
    "    if not 'lemmas' in cleaned_speeches.columns:\n",
    "        cleaned_speeches['lemmas'] = cleaned_speeches['doc'].apply(get_lemma)\n",
    "    if not 'pos' in cleaned_speeches.columns:\n",
    "        cleaned_speeches['pos'] = cleaned_speeches['doc'].apply(get_pos)\n",
    "    print(\"Added columns for Tokens, Lemmata, and POS.\")\n",
    "    return cleaned_speeches\n",
    "\n",
    "afd_speeches_cleaned = spacy_pipeline(afd_speeches_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing the videos (transcribing)\n",
    "\n",
    "The following code blocks preprocess the uploaded videos. The preprocessing includes the following steps:\n",
    "1. Extracting only the audio from the videos using ffmpeg\n",
    "2. Transcribing the audio using OpenAI's Whisper\n",
    "3. Building a dataframe with the transcribed text and further information about the corresponding videos\n",
    "4. Saving the dataframe to a CSV file and pickling it for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following code block extracts the audio from the videos in the raw data folder and saves them as mp3 files in the same folder.\n",
    "This is done to make the audio files accessible for the whisper model.\n",
    "\n",
    "This operation should also be done only once, as it takes some time to extract the audio from the videos.\n",
    "\"\"\"\n",
    "path = \"data/raw_data/videos/YouTube/\"\n",
    "for j in os.listdir(path):\n",
    "    for i in os.listdir(path + j):\n",
    "        if i.endswith(\".webm\") or i.endswith(\".mp4\"):\n",
    "            # Extract the audio from the video\n",
    "            command = \"ffmpeg -i {} -vn -ar 44100 -ac 2 -b:a 192k {}\".format('\"'+path+j+'/'+i+'\"', ('\"'+path+j+'/'+i+'\"').replace(\".mp4\", \".mp3\").replace(\".webm\", \".mp3\"))\n",
    "            subprocess.call(command, shell = True)\n",
    "\n",
    "path = \"data/raw_data/videos/TikTok/\"\n",
    "for i in os.listdir(path):\n",
    "    if i.endswith(\".webm\") or i.endswith(\".mp4\"):\n",
    "        # Extract the audio from the video\n",
    "        command = \"ffmpeg -i {} -vn -ar 44100 -ac 2 -b:a 192k {}\".format('\"'+path+i+'\"', ('\"'+path+i+'\"').replace(\".mp4\", \".mp3\").replace(\".webm\", \".mp3\"))\n",
    "        subprocess.call(command, shell = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following code block transcribes the audio files in the raw data folder and saves them as text files in the same folder.\n",
    "\n",
    "!!!This operation should also be done only once, as it takes much time to transcribe the audio files.!!!\n",
    "\"\"\"\n",
    "def transcribe_audio_files(paths_to_folders):\n",
    "    for path_to_folder in paths_to_folders:\n",
    "        for i in os.listdir(path_to_folder):\n",
    "            if i.endswith(\".mp3\"):\n",
    "                # Checks if the file has already been transcribed\n",
    "                if not os.path.exists(path_to_folder+i.replace(\".mp3\", \".txt\")):\n",
    "                # Transcribe the audio file\n",
    "                    result = model.transcribe(audio = path_to_folder+i, language = \"de\")\n",
    "                    with open(path_to_folder+i.replace(\".mp3\", \".txt\"), \"w\") as f:\n",
    "                        f.write(str(result))\n",
    "        print(\"Transcription of audio files in {} completed.\".format(path_to_folder))\n",
    "\n",
    "transcribe_audio_files(paths_to_folders)\n",
    "\n",
    "\"\"\"\n",
    "The following method builds a dataframe from the transcriptions in the txt in the raw data folder.\n",
    "\n",
    "@TODO: Reproducaibility of the code block. The videofiles have to be reloaded into the folder, because ffmpeg changes their \"last modified\" date (at least I think so)\n",
    "\"\"\"\n",
    "\n",
    "def build_transcriptions_df(paths_to_folders):\n",
    "    row_list = []\n",
    "    for path_to_folder in paths_to_folders:\n",
    "        for i in os.listdir(path_to_folder):\n",
    "            if i.endswith(\".txt\"):\n",
    "                with open(path_to_folder+i, \"r\") as f:\n",
    "                    data = f.read()\n",
    "                # Dict in string -> dict and extract the transcription \n",
    "                transcription = ast.literal_eval(data)['text']\n",
    "                # Extract the source from the path\n",
    "                source = path_to_folder.split('/')[-2]\n",
    "                # Get the time of the video file. It is the time of the last modification of the video file. You have to try different file endings.\n",
    "                # @TODO: Program a preprocessing stage where every file is changed to mp4\n",
    "                try:\n",
    "                    time = strftime('%Y-%m-%d %H:%M:%S', localtime(os.path.getmtime((path_to_folder+i).replace(\".txt\", \".webm\"))))\n",
    "                except:\n",
    "                    try:\n",
    "                        time = strftime('%Y-%m-%d %H:%M:%S', localtime(os.path.getmtime((path_to_folder+i).replace(\".txt\", \".mp4\"))))\n",
    "                    except:\n",
    "                        time = \"unknown\"\n",
    "                # Put the transcription and source in the dataframe\n",
    "                row_list.append({'text': transcription, 'source': source, 'time': time})\n",
    "    return row_list\n",
    "\n",
    "transcriptions_df = pd.DataFrame(build_transcriptions_df(paths_to_folders))\n",
    "\n",
    "transcriptions_df.to_csv(\"data/preprocessed_data/transcriptions.csv\")\n",
    "transcriptions_df.to_pickle(\"data/preprocessed_data/transcriptions.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Matching the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method aligns the transcriptions with the speeches from the protocols.\n",
    "It does this by comparing the lemmata of the speeches with the transcriptions and finding the most similar transcription. \n",
    "The lemmata are used because they are more robust to spelling errors and other noise in the text. All in all the transcriptions are pretty good\n",
    "so this may not be strictly necessary.\n",
    "\n",
    "@Input:\n",
    "    transcriptions_df: The dataframe with the transcriptions\n",
    "    afd_speeches_cleaned: The dataframe with the speeches from the protocols\n",
    "@Output:\n",
    "    The protool dataframe (2nd argument) with the transcriptions added as a column\n",
    "\"\"\"\n",
    "def text_alignment(transcriptions_df: pd.DataFrame, afd_speeches_cleaned: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Align the transcriptions with the speeches from the protcols"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

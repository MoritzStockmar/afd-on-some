{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"The 'Alternative für Deutschland' and their usage of parliamentary speeches on Social Media\"\n",
    "\n",
    "## Information about the research project\n",
    "\n",
    "In this project, which is the Masters Thesis of Moritz Stockmar, the usage of parliamentary speeches by the 'Alternative für Deutschland' (AfD) on Social Media is analyzed. The main objective is to learn which speeches (and parts thereof) are used by the AfD on TikTok and YouTube Shorts, what differentiates them form the population of all of AfD's parliamentary speeches.\n",
    "\n",
    "To achieve this goal the following steps are taken:\n",
    "\n",
    "1. Collecting the data: The speeches of the AfD are collected from the plenary protocolls which can be found on the official website of the Bundestag. The TikTok and YouTube Shorts videos are collected from official social media accounts of the AfD.\n",
    "\n",
    "2. Preprocessing the data: A corpus of AfD speeches during the 20. legislative period of the Bundestag is built. The short videos are transcribed and matched to the corpus entries.\n",
    "\n",
    "3. Analyzing the data: After alligning the uploaded speeches with the official parliamentary protocols, the speeches are analyzed using a variety of methods, such as topic modeling, sentiment analysis, syntactical analysis and more.\n",
    "\n",
    "4. Visualising the data: The results are visualized to be presented in the Masters Thesis.\n",
    "\n",
    "The steps above are also reflected in the structure of this md-document. Refer to the respective sections for more detailled information.\n",
    "The data was (will be) collected by the author and is available on request. The goal is, that this markdown will be self contained and can be used to reproduce the results of the thesis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setting global options and loading the required libraries\n",
    "The following sections load the required libraries and sets some global variables. This sections must be run before any other section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@TODO: The following code builds the recquired file structure for the project.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import bundestag_api\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import whisper\n",
    "\n",
    "import subprocess\n",
    "import ast\n",
    "import unicodedata\n",
    "\n",
    "from spacy import displacy\n",
    "from time import strftime, localtime\n",
    "\n",
    "# Dowmload the required models\n",
    "\n",
    "# Download and load the spaCy model\n",
    "#!python -m spacy download de_core_news_md\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "# @TODO: Trying it with a bigger model\n",
    "\n",
    "\n",
    "# Whisper: The Turbo Model is used. It is 1.5 GB big in storage and uses roughly 6 GB of RAM. \n",
    "# https://huggingface.co/openai/whisper-large-v3-turbo\n",
    "model = whisper.load_model(\"turbo\")\n",
    "# Hugging Face Models\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "@TODO: The following code builds the recquired file structure for the project.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setting global variables\n",
    "BUNDESTAG_API = \"I9FKdCn.hbfefNWCY336dL6x62vfwNKpoN2RZ1gp21\"\n",
    "period_start = '2021-10-01'\n",
    "period_end = '2025-03-24'\n",
    "\n",
    "path_yt_faction = \"data/raw_data/videos/YouTube/AfD-Fraktion Bundestag (Parliamentary Faction)/\"\n",
    "path_yt_party = \"data/raw_data/videos/YouTube/AfD-TV (Party)/\"\n",
    "path_tt = \"data/raw_data/videos/TikTok/\"\n",
    "path_insta_faction = \"data/raw_data/videos/Instagram/InstaFaction/\"\n",
    "path_insta_party = \"data/raw_data/videos/Instagram/InstaParty/\"\n",
    "paths_to_folders = [path_yt_faction, path_yt_party, path_tt, path_insta_faction, path_insta_party]\n",
    "\n",
    "parties = [\"CDU/CSU\", \"SPD\", \"AfD\", \"FDP\", \"BÜNDNIS 90/DIE GRÜNEN\", \"Die Linke\", \"BSW\" \"fraktionslos\"]\n",
    "länder = [\"Baden-Württemberg\", \"Bayern\", \"Berlin\", \"Brandenburg\", \"Bremen\", \"Hamburg\", \"Hessen\", \"Mecklenburg-Vorpommern\", \"Niedersachsen\", \"Nordrhein-Westfalen\", \"Rheinland-Pfalz\", \"Saarland\", \"Sachsen\", \"Sachsen-Anhalt\", \"Schleswig-Holstein\", \"Thüringen\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collecting the Data\n",
    "The plenary protocolls are collected using the [official API](https://dip.bundestag.de/%C3%BCber-dip/hilfe/api) of the German Bundestag. The [python-wrapper](https://github.com/jschibberges/Bundestag-API) by jschibberges is used to do it. The short videos were downloaded via the tool [youtube-dlp](https://github.com/yt-dlp/yt-dlp). This won't be shown here but the video database can be accessed on request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Downloading the speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Collecting the protocols from the Bundestag API from the first session of the 20th legislative period \n",
    "to the date of the election of the 21st legislative period. \n",
    "\n",
    "This codeblock takes some time to run as it collects the data from the Bundestag API. There are 214 protocols to download. \n",
    "Only do this once, as the data is saved to a CSV file for long term storage and the dataframe is pickled for shorter term storage.\n",
    "\n",
    "@TODO: Add the sessions after the election of the 21st legislative period to the data collection\n",
    "\"\"\"\n",
    "bta = bundestag_api.btaConnection(apikey = BUNDESTAG_API)\n",
    "protocols = bta.search_plenaryprotocol(date_start = period_start, date_end = period_end, institution = 'BT', num = 400, fulltext = True)\n",
    "protocols_df  = pd.DataFrame(protocols)\n",
    "\n",
    "# Save the dataframe\n",
    "protocols_df.to_csv(\"data/raw_data/protocols.csv\")\n",
    "protocols_df.to_pickle(\"data/raw_data/protocols.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Downloading the MdB Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download persons from the Bundestag API. It is not completly clear how search_person works, therefore we will download all persons and filter them later.\n",
    "bta = bundestag_api.btaConnection(apikey = BUNDESTAG_API)\n",
    "persons = bta.search_person(updated_since = period_start + \"T00:00:00\", num = 3000)\n",
    "persons_df = pd.DataFrame(persons)\n",
    "\n",
    "# Filter the persons_df for AfD speakers, these can only be MdBs as the AfD does not have any ministers or MdBRs (Memebers of the Bundesrat)\n",
    "# There are a few to many persons which can be explained by people leaving the faction (and now being crossbencher) or leaving parliament during the legislative period\n",
    "filtered_afd_mdbs_df = persons_df[(persons_df['person_roles'].str.contains('AfD', na = False))| \n",
    "                                (persons_df['titel'].str.contains('AfD', na = False))]\n",
    "\n",
    "# Filter the persons_df for non AfD speakers, these can be MdBs, Ministers and MdBRs \n",
    "# There is no need to filter for party other than not being in the AfD\n",
    "filtered_non_afd_speekers_df = persons_df[~((persons_df['person_roles'].str.contains('AfD', na = False))| \n",
    "                                (persons_df['titel'].str.contains('AfD', na = False)))]\n",
    "\n",
    "# Filters everyone out did not speak in the 20th legislative period\n",
    "filtered_non_afd_speekers_df = filtered_non_afd_speekers_df.loc[\n",
    "    # Simple lookup for the wahlperiode == 20 (This applies to everyone speaking for the first time in the 20th legislative period)\n",
    "    (filtered_non_afd_speekers_df['wahlperiode'] == 20) |\n",
    "    # Complicated lookup for everyone else (This applies to everyone who spoke before the 20th legislative period and in the 20th) \n",
    "    (filtered_non_afd_speekers_df['person_roles'].apply(\n",
    "        lambda roles: isinstance(roles, list) and\n",
    "        any(20 in role.get('wahlperiode_nummer', []) for role in roles if isinstance(role, dict))\n",
    "    ))\n",
    "]\n",
    "\n",
    "# Save the dataframe\n",
    "filtered_afd_mdbs_df.to_csv(\"data/raw_data/afd_mdbs.csv\")\n",
    "filtered_afd_mdbs_df.to_pickle(\"data/raw_data/afd_mdbs.pkl\")\n",
    "\n",
    "filtered_non_afd_speekers_df.to_csv(\"data/raw_data/non_afd_speakers.csv\")\n",
    "filtered_non_afd_speekers_df.to_pickle(\"data/raw_data/non_afd_speakers.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the data\n",
    "\n",
    "In the following the preprocessing of the textual and the audio-visual data is performed. \n",
    "\n",
    "1. The textual data (the protocols) is processed into a annotated Corpus consisting of all speeches by AfD MdBs (members of the Bundestag). Therefor firstly all speeches of AfD MdBs have to be extracted (in plain text) from the protocols. The next step is the linguistical preprocessing performed by spaCy to make the data usable for the upcoming syntactical and semantical examinations.\n",
    "\n",
    "2. The audio-visual data (the uploaded short videos) have to be transcribed.\n",
    "\n",
    "3. The transcriptions are matched with their corresponding speeches from the frist step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preprocessing the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Extracting the AfD Speeches from the protocols\n",
    "\n",
    "The following codeblock extracts the AfD Speeches from the protocols and stores them in a single dataframe\n",
    "\n",
    "Attention: This block takes roughly 30s to complete on a M2 Mac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following code block reads the protocols from the CSV file / pickeled file and loads them into a pandas dataframe for further processing\n",
    "if there is no dataframe already created.\n",
    "\"\"\"\n",
    "if os.path.exists(\"data/raw_data/protocols.pkl\"):\n",
    "    protocols_df = pd.read_pickle(\"data/raw_data/protocols.pkl\")\n",
    "elif os.path.exists(\"data/raw_data/protocols.csv\"):\n",
    "    protocols_df = pd.read_csv(\"data/raw_data/protocols.csv\")\n",
    "else:\n",
    "    print(\"No data found. Please run the data collection code block first.\")\n",
    "\n",
    "\"\"\"The afd_mdbs_searchstrings are the strings signaling the start of every AfD Speech in the protocols by being of the form\n",
    "'[o: Titels] [first name] [o: infix] [last name], (AfD):' -> things in brackets are individual, things with o: are optional\n",
    "This searchstring can be used as is to find the start of the speech in the protocol \"\"\"\n",
    "afd_mdbs_searchstrings = filtered_afd_mdbs_df.apply(lambda row: row['titel'].replace(', MdB', '').replace('Dr. ', '').replace(', AfD', ' (AfD)') + \":\", axis = 1).to_list()\n",
    "\n",
    "\"\"\"The non_afd_searchstring is part of the string signaling the start of every non AfD speech in the protocols by being of the form\n",
    "'[o: Titels] [first name] [o: infix] [last name]', The party or affiliation to the federal government or bundesrat is not included, because that would lead to many edge cases\n",
    "This searchsting can NOT be used as is to find the start of the speech in the protocol \"\"\"\n",
    "non_afd_searchstings = filtered_non_afd_speekers_df.apply(lambda row: row['titel'].split(',')[0], axis = 1).to_list()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Helper method to check if a line is the start of an AfD speech. This is done by checking if the line contains any of the searchstrings\n",
    "and if the line does not contain the word \"Frage\" which indicates that the line is a purely written question and not a speech. \n",
    "These are found at the end of protocols if the time was to short to answer each question for the government orally.\n",
    "\"\"\"\n",
    "def is_start_of_AfDSpeech(line, searchstrings) -> bool:\n",
    "    return any(searchstring in line for searchstring in searchstrings) and not \"Frage\" in line\n",
    "\n",
    "\"\"\"\n",
    "Second Helper method to check if a line is the start of an AfD speech which handels the case where the speaker announcement in the protocol\n",
    "is split into two lines. It is only checked for \"(AfD):\" and the last name of the speaker. \n",
    "\n",
    "There are two known special cases: In the first case the line is split after the first name and the second case is that the line is split after the last name.\n",
    "\"\"\"\n",
    "def is_start_of_AfDSpeech_split_name(line, next_line, afd_mdbs_searchstrings) -> bool:\n",
    "    if \"(AfD):\" in line and not \"Frage\" in line and any(nachname in line for nachname in filtered_afd_mdbs_df['nachname'].to_list()):\n",
    "        return True\n",
    "    if \"(AfD):\" in next_line and not \"Frage\" in next_line and any(nachname in line for nachname in filtered_afd_mdbs_df['nachname'].to_list()) and not is_start_of_AfDSpeech(next_line, afd_mdbs_searchstrings):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\"\"\"\n",
    "Helper method to check if a line is the end of an AfD speech. This is done by checking if the line contains any of the searchstrings\n",
    "and if the line ends with a colon as otherwise references to people would be falsely identified as the end of a speech. Furthermore\n",
    "the line should not start with a bracket as this indicates comments from the audience.\n",
    "\"\"\"\n",
    "def is_end_of_AfDSpeech(line, searchstrings) -> bool:\n",
    "    return any(searchstring in line for searchstring in searchstrings) and line[-1] == ':' and line[0] != '['\n",
    "\n",
    "\"\"\"\n",
    "Helper method to finde the session (as in X. Sitzung der 20. Wahlperiode) and the date of the session in the protocol.\n",
    "@Input: protocol: The protocol as a list of strings (lines). You only need the first 5\n",
    "@Output: A tuple of the form (session, session_date)\n",
    "\"\"\"\n",
    "\"\"\" def find_session_and_date(protocol_lines) -> tuple:\n",
    "    session = protocol_lines[0].split('/')[-1] # The session is after the last '/' in the first line\n",
    "    session_date = protocol_lines[4].split(',')[-1].replace(' den ', '') # The date is after the last ', den ' in the fifth line\n",
    "    return (session, session_date) \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Helper method to find the start of a agenda item \n",
    "\"\"\"\n",
    "def is_start_of_agenda_item(line) -> bool:\n",
    "    line = line.lower()\n",
    "    # The agenda item is always after something like \"ich rufe\" or \"wir kommen\" and contains either \"Tagesordnungspunkt\" or \"Zusatzpunkt\"\n",
    "    # Logical structure (expression for calling new agenda item) and (numercial expression for agenda item)\n",
    "    return (\"rufe\" in line or \"kommen\" in line or \"komme\" in line or bool(re.search('setzen (.*) fort', line))) and (bool(re.search('(tagesordnungspunkte?|zusatzpunkte?) [0-9]?[0-9]?', line)))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Main method to extract the speeches from a protocol. \n",
    "@Input: protocol: The protocol as a string\n",
    "        afd_mdbs_searchstrings: The searchstrings to identify the start of an AfD speech\n",
    "        non_afd_searchstrings: The searchstrings to identify the end of an AfD speech\n",
    "@Output: A Dataframe of the form {'speaker': speaker, 'text': text, 'session': session, 'session_date': session_date, 'agenda_item': agenda_item}\n",
    "\n",
    "The method works by iterating over the normalized lines of the protocol and checking if a line is the start of an AfD speech. If it is, the method\n",
    "starts to collect the text of the speech until it finds the end of the speech. \n",
    "\n",
    "Afterwards a first structural cleanup is done. This means that speeches that are split due to interventions by the prisiding officer (espacially because of time) are concatenated.\n",
    "\n",
    "@TODO: agenda_items is not working correctly yet. It is sometimes empty because of the hard coded way of finding the agenda item.\n",
    "\"\"\"\n",
    "def extract_speeches_from_protocol(protocol, session, date, afd_mdbs_searchstrings, non_afd_searchstings) -> pd.DataFrame:\n",
    "    protocol_lines = protocol.split('\\n')\n",
    "\n",
    "    # First cleanup: Text normalization\n",
    "    normailzed_protocol_lines = [unicodedata.normalize(\"NFKC\", line) for line in protocol_lines]\n",
    "\n",
    "    speeches = []\n",
    "    speech_text = \"\"\n",
    "    in_speech = False\n",
    "    speaker = \"\"\n",
    "    agenda_item = \"\"\n",
    "\n",
    "    for index, line in enumerate(normailzed_protocol_lines):\n",
    "        found = False\n",
    "        if line == \"\": # Skip empty lines alltogether\n",
    "            continue\n",
    "        if is_start_of_agenda_item(line):\n",
    "            i = 1\n",
    "            while(normailzed_protocol_lines[normailzed_protocol_lines.index(line) + i] == \"\"): #Skip empty lines between the saying \"Ich rufe...\" and the name of the\n",
    "                i += 1\n",
    "            agenda_item = normailzed_protocol_lines[normailzed_protocol_lines.index(line) + i]\n",
    "        if in_speech and is_end_of_AfDSpeech(line, non_afd_searchstings):\n",
    "            speeches.append({\n",
    "                'speaker': speaker, \n",
    "                'text': speech_text, \n",
    "                'session': session, \n",
    "                'session_date': date, \n",
    "                'agenda_item': agenda_item}) # Append the speech (dict) to the list of speeches\n",
    "            speech_text, speaker = \"\", \"\" # Reset the variables\n",
    "            in_speech = False\n",
    "        if in_speech:\n",
    "            speech_text += line\n",
    "        if not in_speech and is_start_of_AfDSpeech(line, afd_mdbs_searchstrings):\n",
    "            in_speech = True\n",
    "            speaker = line.replace(\" (AfD):\", \"\")\n",
    "            found = True\n",
    "        # This is a special case for the name of the speaker being split over two lines and the next_line containing the last name or just '(AfD):'\n",
    "        if index < len(normailzed_protocol_lines) - 1:\n",
    "            if not in_speech and not found and is_start_of_AfDSpeech_split_name(line, normailzed_protocol_lines[index + 1], afd_mdbs_searchstrings):\n",
    "                print(\"Found split name\")\n",
    "                in_speech = True\n",
    "                speaker = line.split(\" (AfD):\")[0]\n",
    "\n",
    "    # Structural Cleanup: Concat speeches that are split due to interventions by the speaker due to time\n",
    "    cleaned_speeches = []\n",
    "    for speech in speeches:\n",
    "        try:\n",
    "            if speech['text'].startswith('–'): # Sometimes '-' are used to indicate that a speech is split\n",
    "                cleaned_speeches[-1]['text'] += speech['text']\n",
    "                cleaned_speeches[-1]['text'].replace('––', ' ')\n",
    "            elif len(speech['text'].split(\" \")) < 40: # This is a heuristic to determine if the speech is split because of an intervention because of time, \n",
    "                #as the rest of a speech after an intervention is usually shorter than 40 words and a new speech certainly would be longer\n",
    "                cleaned_speeches[-1]['text'] += \" \" + speech['text']\n",
    "            else:\n",
    "                cleaned_speeches.append(speech)\n",
    "        except:\n",
    "            cleaned_speeches.append(speech)\n",
    "    return pd.DataFrame(cleaned_speeches)\n",
    "\n",
    "#Function call and data storage, takes roughly 30 seconds to run\n",
    "afd_speeches = pd.DataFrame()\n",
    "for _, row in protocols_df.iterrows():\n",
    "    # Get list of dict\n",
    "    extracted_speeches = extract_speeches_from_protocol(\n",
    "        protocol = row['text'], \n",
    "        session = row['dokumentnummer'].split('/')[1],\n",
    "        date = row['datum'], \n",
    "        afd_mdbs_searchstrings = afd_mdbs_searchstrings, \n",
    "        non_afd_searchstings = non_afd_searchstings)\n",
    "\n",
    "    # Convert list of dict to dataframe\n",
    "    afd_speeches = pd.concat([afd_speeches, extracted_speeches], ignore_index = True)\n",
    "\n",
    "afd_speeches.to_csv(\"data/preprocessed_data/afd_speeches.csv\")\n",
    "afd_speeches.to_pickle(\"data/preprocessed_data/afd_speeches.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(protocols_df.iloc[204]['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Bulding the annotated corpus of AfD speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following code block reads the speeches of the AfD MdBs from the CSV file / pickeled file and loads them into a pandas dataframe for further processing\n",
    "if there is no dataframe already created.\n",
    "\"\"\"\n",
    "\n",
    "if os.path.exists(\"data/preprocessed_data/afd_speeches.pkl\"):\n",
    "    afd_speeches = pd.read_pickle(\"data/preprocessed_data/afd_speeches.pkl\")\n",
    "    print(\"Data loaded.\")\n",
    "elif os.path.exists(\"data/preprocessed_data/afd_speeches.csv\"):\n",
    "    afd_speeches = pd.read_csv(\"data/preprocessed_data/afd_speeches.csv\")\n",
    "    print(\"Data loaded.\")\n",
    "else: \n",
    "    print(\"No data found. Please run the data collection code block first.\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The followng code block is used to clean up the speeches further.\n",
    "Cleand up things:\n",
    " - Interjections by the listeners\n",
    " - The (AfD): from the text, they appear in the special cases from the start of speech recognition\n",
    " - The speech by Alice Weidel on the 11.02.2025 is completly butchered \n",
    "\"\"\"\n",
    "\n",
    "def further_cleanup(speeches: pd.DataFrame) -> pd.DataFrame:\n",
    "    cleaned_speeches = speeches.copy()\n",
    "    # Remove interjections by the listeners, these are idicated by brackets in the protocol. As far as I can tell,\n",
    "    # this is the only place where brackets are used in a speech.\n",
    "    cleaned_speeches['text'] = cleaned_speeches['text'].apply(lambda x: re.sub(r'\\(.*?\\)', '', x))\n",
    "    # Remove the (AfD): from the text, they appear in the special cases from the start of speech recognition\n",
    "    cleaned_speeches['text'] = cleaned_speeches['text'].apply(lambda x: x.replace('(AfD):', '')) \n",
    "    # Reconnect the speech by Alice Weidel on the 11.02.2025\n",
    "    cleaned_speeches.iloc[13]['text'] += \" \" + cleaned_speeches.iloc[14]['text'] + \" \" + cleaned_speeches.iloc[15]['text']\n",
    "    cleaned_speeches.drop([14, 15], inplace = True)\n",
    "    print(\"Cleaned up the speeches.\")\n",
    "    return cleaned_speeches\n",
    "\n",
    "\n",
    "\n",
    "afd_speeches_cleaned = further_cleanup(afd_speeches)\n",
    "\n",
    "\"\"\"\n",
    "Some helper methods to simplfy the .apply statements in the following code block\n",
    "\"\"\"\n",
    "def get_token(doc):\n",
    "    return [token.text for token in doc]\n",
    "def get_lemma(doc):\n",
    "    return [token.lemma_ for token in doc]\n",
    "def get_pos(doc):\n",
    "    return [(token.pos_, token.tag_) for token in doc]\n",
    "\n",
    "# Preprocessing the speeches with the spaCy pipeline (tokenization, lemmatization, POS tagging, etc. pp.)\n",
    "# This takes some time to run (with an Apple M2 the pipeline itself (building the doc) took around 2 minutes)\n",
    "\n",
    "def spacy_pipeline(cleaned_speeches: pd.DataFrame, text_column) -> pd.DataFrame:\n",
    "    if not 'doc' in cleaned_speeches.columns:\n",
    "       cleaned_speeches['doc'] = cleaned_speeches[text_column].apply(nlp)\n",
    "    print(\"Pipeline finished.\")\n",
    "    if not 'tokens' in cleaned_speeches.columns:\n",
    "        cleaned_speeches['tokens'] = cleaned_speeches['doc'].apply(get_token)\n",
    "    if not 'lemmas' in cleaned_speeches.columns:\n",
    "        cleaned_speeches['lemmas'] = cleaned_speeches['doc'].apply(get_lemma)\n",
    "    if not 'pos' in cleaned_speeches.columns:\n",
    "        cleaned_speeches['pos'] = cleaned_speeches['doc'].apply(get_pos)\n",
    "    print(\"Added columns for Tokens, Lemmata, and POS.\")\n",
    "    return cleaned_speeches\n",
    "\n",
    "afd_speeches_cleaned = spacy_pipeline(afd_speeches_cleaned, 'text')\n",
    "\n",
    "afd_speeches_cleaned.to_pickle(\"data/preprocessed_data/afd_speeches_cleaned.pkl\")\n",
    "afd_speeches_cleaned.to_csv(\"data/preprocessed_data/afd_speeches_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing the videos (transcribing)\n",
    "\n",
    "The following code blocks preprocess the uploaded videos. The preprocessing includes the following steps:\n",
    "1. Extracting only the audio from the videos using ffmpeg\n",
    "2. Transcribing the audio using OpenAI's Whisper\n",
    "3. Building a dataframe with the transcribed text and further information about the corresponding videos\n",
    "4. Saving the dataframe to a CSV file and pickling it for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following code block extracts the audio from the videos in the raw data folder and saves them as mp3 files in the same folder.\n",
    "This is done to make the audio files accessible for the whisper model.\n",
    "\n",
    "This operation should also be done only once, as it takes some time to extract the audio from the videos.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Takes a path to a folder and extracts the audio from the videos in the folder and saves them as mp3 files in the same folder.\n",
    "\n",
    "@Input: path: The path to the folder with the videos. The videos should be in the formats .mp4 and .webm. \n",
    "There can be one layer of subfolders in the folder (party and faction)\n",
    "\"\"\"\n",
    "def make_mp4(path: str):\n",
    "    for i in os.listdir(path):\n",
    "        if i.endswith(\".webm\") or i.endswith(\".mp4\"):\n",
    "            # Extract the audio from the video\n",
    "            command = \"ffmpeg -i {} -vn -ar 44100 -ac 2 -b:a 192k {}\".format('\"'+path+i+'\"', ('\"'+path+i+'\"').replace(\".mp4\", \".mp3\").replace(\".webm\", \".mp3\"))\n",
    "            subprocess.call(command, shell = True)\n",
    "\n",
    "make_mp4(\"data/raw_data/videos/Instagram/Faction/\")\n",
    "make_mp4(\"data/raw_data/videos/Instagram/Party/\")\n",
    "#make_mp4(\"data/raw_data/videos/TikTok/\")\n",
    "#make_mp4(\"data/raw_data/videos/YouTube/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following code block transcribes the audio files in the raw data folder and saves them as text files in the same folder.\n",
    "\n",
    "!!!This operation should also be done only once, as it takes much time to transcribe the audio files.!!!\n",
    "\"\"\"\n",
    "def transcribe_audio_files(paths_to_folders):\n",
    "    for path_to_folder in paths_to_folders:\n",
    "        for i in os.listdir(path_to_folder):\n",
    "            if i.endswith(\".mp3\"):\n",
    "                # Checks if the file has already been transcribed\n",
    "                if not os.path.exists(path_to_folder+i.replace(\".mp3\", \".txt\")):\n",
    "                # Transcribe the audio file\n",
    "                    result = model.transcribe(audio = path_to_folder+i, language = \"de\")\n",
    "                    with open(path_to_folder+i.replace(\".mp3\", \".txt\"), \"w\") as f:\n",
    "                        f.write(str(result))\n",
    "        print(\"Transcription of audio files in {} completed.\".format(path_to_folder))\n",
    "\n",
    "transcribe_audio_files(paths_to_folders)\n",
    "\n",
    "\"\"\"\n",
    "The following method builds a dataframe from the transcriptions in the txt in the raw data folder.\n",
    "\n",
    "@TODO: Reproducaibility of the code block. The videofiles have to be reloaded into the folder, because ffmpeg changes their \"last modified\" date (at least I think so)\n",
    "\"\"\"\n",
    "\n",
    "def build_transcriptions_df(paths_to_folders):\n",
    "    row_list = []\n",
    "    for path_to_folder in paths_to_folders:\n",
    "        for i in os.listdir(path_to_folder):\n",
    "            if i.endswith(\".txt\"):\n",
    "                with open(path_to_folder+i, \"r\") as f:\n",
    "                    data = f.read()\n",
    "                # Dict in string -> dict and extract the transcription \n",
    "                transcription = ast.literal_eval(data)['text']\n",
    "                # Extract the source from the path\n",
    "                source = path_to_folder.split('/')[-2]\n",
    "                # Get the time of the video file. It is the time of the last modification of the video file. You have to try different file endings.\n",
    "                # @TODO: Program a preprocessing stage where every file is changed to mp4\n",
    "                if bool(re.search('202[1-5]-[0-1][0-9]-[0-3][0-9]', i)):\n",
    "                    time = re.search('202[1-5]-[0-1][0-9]-[0-3][0-9]', i).group(0)\n",
    "                else:\n",
    "                    try:\n",
    "                        time = strftime('%Y-%m-%d %H:%M:%S', localtime(os.path.getmtime((path_to_folder+i).replace(\".txt\", \".webm\"))))\n",
    "                    except:\n",
    "                        try:\n",
    "                            time = strftime('%Y-%m-%d %H:%M:%S', localtime(os.path.getmtime((path_to_folder+i).replace(\".txt\", \".mp4\"))))\n",
    "                        except:\n",
    "                            time = \"unknown\"\n",
    "                # Put the transcription and source in the dataframe\n",
    "                row_list.append({'text': transcription, 'source': source, 'time': pd.to_datetime(time)})\n",
    "    return row_list\n",
    "\n",
    "all_transcriptions_df = pd.DataFrame(build_transcriptions_df(paths_to_folders))\n",
    "\n",
    "# Filter Dataframe so that only videos uploaded during the 20. legislative period are included (25.10.2021 - 24.03.2025)\n",
    "# There still might be a handfull of videos that are from the 19. legislative period\n",
    "transcriptions_df = all_transcriptions_df.loc[all_transcriptions_df['time'] > pd.to_datetime('2021-10-25')]\n",
    "transcriptions_df = transcriptions_df.loc[transcriptions_df['time'] < pd.to_datetime('2025-03-24')]\n",
    "\n",
    "transcriptions_df.to_csv(\"data/preprocessed_data/transcriptions.csv\")\n",
    "transcriptions_df.to_pickle(\"data/preprocessed_data/transcriptions.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In the following the duplicates in the transcription dataframe are removed.\n",
    "There are duplicates because some Videos were downloaded on YouTube and on TikTok\n",
    "\"\"\"\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "if os.path.exists(\"data/preprocessed_data/transcriptions.pkl\"):\n",
    "    transcriptions_df = pd.read_pickle(\"data/preprocessed_data/transcriptions.pkl\")\n",
    "elif os.path.exists(\"data/preprocessed_data/transcriptions.csv\"):\n",
    "    transcriptions_df = pd.read_csv(\"data/preprocessed_data/transcriptions.csv\")\n",
    "else:\n",
    "    print(\"No data found. Please run the data collection code block first.\")\n",
    "\n",
    "def remove_duplicates(dup_df, text_column, source_column, similarity_treshold = 80) -> pd.DataFrame:\n",
    "    # Remove leading and trailing whitespaces\n",
    "    dup_df['text'] = dup_df['text'].apply(str.strip)\n",
    "    # Lemmatize the text for better comparison\n",
    "    dup_df['doc'] = dup_df['text'].apply(nlp)\n",
    "    dup_df['lemma'] = dup_df['doc'].apply(get_lemma)\n",
    "\n",
    "    dup_df.reset_index(drop = True)\n",
    "\n",
    "    seen_rows = []\n",
    "    duplicates = []\n",
    "\n",
    "    dup_df['dup_num'] = 0\n",
    "    i = 1\n",
    "\n",
    "    for _, row in dup_df.iterrows():\n",
    "        for seen_row in seen_rows:\n",
    "            if fuzz.ratio(row[text_column], seen_row[text_column]) > similarity_treshold:\n",
    "                row['dup_num'] = i\n",
    "                seen_row['dup_num'] = i\n",
    "                duplicates.append(row)\n",
    "                i += 1\n",
    "                break\n",
    "        seen_rows.append(row)\n",
    "    \n",
    "    duplicates_df =  pd.DataFrame(duplicates)\n",
    "    dup_df = pd.DataFrame(seen_rows)\n",
    "    \n",
    "    grouped_sources = duplicates_df.groupby('dup_num')['source'].apply(lambda x: ','.join(x)).to_dict()\n",
    "    for dup_num, sources in grouped_sources.items():\n",
    "        dup_df.loc[dup_df['dup_num'] == dup_num, 'source'] += f\",{sources}\"\n",
    "\n",
    "\n",
    "    no_dup = dup_df.drop(duplicates_df.index)\n",
    "    return no_dup, duplicates_df\n",
    "\n",
    "transcriptions_no_dup_df, duplicates_df = remove_duplicates(transcriptions_df, 'text', 'source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(dup_df, text_column, source_column, similarity_treshold=80) -> pd.DataFrame:\n",
    "    # Remove leading and trailing whitespaces\n",
    "    dup_df['text'] = dup_df['text'].apply(str.strip)\n",
    "    # Lemmatize the text for better comparison\n",
    "    dup_df['doc'] = dup_df['text'].apply(nlp)\n",
    "    dup_df['lemma'] = dup_df['doc'].apply(get_lemma)\n",
    "\n",
    "    # Reset index for consistency\n",
    "    dup_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Initialize variables for tracking duplicates\n",
    "    seen_rows = []\n",
    "    duplicates = []\n",
    "    dup_df['dup_num'] = '0'  # Initialize a column for duplicate group numbers\n",
    "    current_dup_num = 1  # Start numbering duplicate groups from 1\n",
    "\n",
    "    # Iterate through rows to find duplicates\n",
    "    for _, row in dup_df.iterrows():\n",
    "        matched = False\n",
    "        for seen_row in seen_rows:\n",
    "            if fuzz.ratio(row[text_column], seen_row[text_column]) > similarity_treshold:\n",
    "                # Assign the same dup_num to the current row and the matched row\n",
    "                row['dup_num'] += ',' + seen_row['dup_num']\n",
    "                duplicates.append(row)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            # If no match is found, assign a new dup_num\n",
    "            row['dup_num'] = str(current_dup_num)\n",
    "            current_dup_num += 1\n",
    "        seen_rows.append(row)\n",
    "\n",
    "    # Convert the duplicates list to a DataFrame\n",
    "    dup_df = pd.DataFrame(seen_rows)\n",
    "\n",
    "    dup_df['further times'] = \"\"\n",
    "\n",
    "    leading_zero_rows = dup_df[dup_df['dup_num'].str.startswith('0')]\n",
    "    non_zero_rows = dup_df[~dup_df['dup_num'].str.startswith('0')]\n",
    "\n",
    "     # Iterate through rows with leading 0 and update the corresponding non-zero row\n",
    "    for _, row in leading_zero_rows.iterrows():\n",
    "        # Remove the leading 0 from dup_num\n",
    "        corrected_dup_num = row['dup_num'].lstrip('0,')\n",
    "\n",
    "        # Find the corresponding row in non_zero_rows\n",
    "        if corrected_dup_num in non_zero_rows['dup_num'].values:\n",
    "            # Concatenate the source of the leading 0 row to the non-zero row\n",
    "            non_zero_rows.loc[non_zero_rows['dup_num'] == corrected_dup_num, source_column] += f\",{row[source_column]}\"\n",
    "            non_zero_rows.loc[non_zero_rows['dup_num'] == corrected_dup_num, 'further times'] += f\",{row['time']}\"\n",
    "\n",
    "    return non_zero_rows, leading_zero_rows \n",
    "\n",
    "# Remove duplicates and update the source column\n",
    "transcriptions_no_dup_df, duplicates_df = remove_duplicates(transcriptions_df, 'text', 'source')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Matching the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method aligns the transcriptions with the speeches from the protocols.\n",
    "It does this by comparing the lemmata of the speeches with the transcriptions and finding the most similar transcription. \n",
    "The lemmata are used because they are more robust to spelling errors and other noise in the text. All in all the transcriptions are pretty good\n",
    "so this may not be strictly necessary.\n",
    "\n",
    "@Input:\n",
    "    speeches_df: The dataframe with the transcriptions\n",
    "    transcriptions_df: The dataframe with the speeches from the protocols\n",
    "@Output:\n",
    "    A dataframe with the speeches and the aligned transcriptions\n",
    "\n",
    "Attention! This code block takes some time to run as it has to compare every speech (> 3500) with some transcription. \n",
    "And the fuzzy matching algorithm is not the fastest. As it must deal with the fact that most uploaded speeches are only \n",
    "subsets of the transcriptions.\n",
    "\"\"\"\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def align_speeches_with_transcriptions(speeches_df, transcriptions_df, treshold = 70):\n",
    "    aligned_data = []\n",
    "    # Sort the dataframes by date to make the search for the best match faster\n",
    "    speeches_df.sort_values(by = 'session_date')\n",
    "    transcriptions_df.sort_values(by = 'time')\n",
    "    num = 0\n",
    "\n",
    "    # The outer for has to be the transcriptions, because sometimes there were multiple videos of different parts of one speech\n",
    "    for _, transcription_row in transcriptions_df.iterrows():\n",
    "        # You dont start with 0, because in the cases, that the whole speech was uploaded the \"Frau Präsidentin! Meine sehr geehrten Damen und Herren!\" \n",
    "        # at the start of it skews the results if you lower the similarity score to much\n",
    "        search_text = transcription_row['text'][150:400]\n",
    "        found_speech, found_speech_date, speaker = None, None, None\n",
    "        transcription_text = transcription_row['text']\n",
    "        transcription_date = pd.to_datetime(transcription_row['time'])\n",
    "        for _, speech_row in speeches_df.iterrows():\n",
    "            speech_text = speech_row['text']\n",
    "            speech_date = pd.to_datetime(speech_row['session_date'])\n",
    "            # Check that the video was uploaded after the speech was given, accelerates the algorithm a bit\n",
    "            if(speech_date <= transcription_date):\n",
    "                similarity_score = fuzz.partial_ratio(speech_text, search_text, score_cutoff = treshold)\n",
    "                if similarity_score > treshold:\n",
    "                    found_speech = speech_text\n",
    "                    found_speech_date = speech_date\n",
    "                    speaker = speech_row['speaker']\n",
    "                    num += 1\n",
    "                    print('Found number {}!'.format(num))\n",
    "                    break\n",
    "        aligned_data.append({\n",
    "            'speech_text': found_speech,\n",
    "            'session_date': found_speech_date,\n",
    "            'transcription_text': transcription_text,\n",
    "            'transcription_date': transcription_date,\n",
    "            'speaker': speaker,\n",
    "            'uploaded_source': transcription_row['source'],\n",
    "        })\n",
    "    return pd.DataFrame(aligned_data)\n",
    "\n",
    "aligned_df = align_speeches_with_transcriptions(afd_speeches_cleaned, transcriptions_no_dup_df)\n",
    "\n",
    "aligned_df.to_csv(\"data/preprocessed_data/aligned_df.csv\")\n",
    "aligned_df.to_pickle(\"data/preprocessed_data/aligned_df.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cleanup: In the following code block the aligned dataframe is cleaned up by hand. \n",
    "\n",
    "This means that obvious misses are corrected every step is documented in the code block.\n",
    "\"\"\"\n",
    "\n",
    "if os.path.exists(\"data/preprocessed_data/aligned_df.pkl\"):\n",
    "    aligned_df_temp = pd.read_pickle(\"data/preprocessed_data/aligned_df.pkl\")\n",
    "elif os.path.exists(\"data/preprocessed_data/aligned_df.csv\"):\n",
    "    aligned_df_temp = pd.read_csv(\"data/preprocessed_data/aligned_df.csv\")\n",
    "else:\n",
    "    print(\"No data found. Please run the alignement code block first.\")\n",
    "\n",
    "\n",
    "# aligned_df = pd.read_pickle(\"data/preprocessed_data/aligned_df.pkl\")\n",
    "#aligned_df = pd.read_csv(\"data/preprocessed_data/aligned_df.csv\")\n",
    "aligned_df_temp.drop(index = 17, inplace = True) # Not an AfD MdB (es unglaublich, was für ein Angriff )\n",
    "aligned_df_temp.drop(index = 218, inplace = True) # This is not a speech by a AfD MdB but an announcement from the speaker (2014, 698 wurden abgegeben...)\n",
    "aligned_df_temp.drop(index = 333, inplace = True) # This is not a speech by an AfD MdB but an announcement from the speaker (Abgegebene Stimmkarten 683)\n",
    "aligned_df_temp.drop(index = 267, inplace = True) # This is not a speech by an AfD MdB (Diese Landtagswahlen in Thüringen)\n",
    "aligned_df_temp.drop(index = 314, inplace = True) # This is not even a speech in the Bundestag (Die Ukraine ist nicht das 17. Bundesland)\n",
    "aligned_df_temp.drop(index = 152, inplace = True) # This speech is not in protocols_df as they forgot to upload a complete version (even if they wrote they would) (Wenn wir mitbekommen, wie in den )\n",
    "aligned_df_temp.drop(index = 382, inplace = True) # This is a debate between an AfD MdB and a minister of state which does not fit the following algorithms (Die fünf wichtigsten und aktuell)\n",
    "aligned_df_temp.drop(index = 420, inplace = True) # This is a debate between an AfD MdB and a minister of state which does not fit the following algorithms (Wer hätte gedacht, dass steigende)\n",
    "aligned_df_temp.drop(index = 92, inplace = True) # No longer an AfD MdB (Die Abschaffung des Netzwerkdurchsetzungsgesetzes)\n",
    "aligned_df_temp.drop(index = 416, inplace = True) # No longer an AfD MdB (Frau Präsidentin, werte Kollegen! Bevor ich)\n",
    "aligned_df_temp.drop(index = 184, inplace = True) # Not an AfD Speaker\n",
    "\n",
    "# This video is more of a compilation: (Die Fahne Deutschlands)\n",
    "aligned_df_temp.at[193,'speech_text'] = afd_speeches_cleaned.iloc[1013]['text']\n",
    "aligned_df_temp.at[193,'session_date'] = afd_speeches_cleaned.iloc[1013]['session_date']\n",
    "aligned_df_temp.at[193, 'speaker'] = afd_speeches_cleaned.iloc[1013]['speaker']\n",
    "aligned_df_temp.at[193,'transcription_text'] = aligned_df_temp.at[193,'transcription_text'].split('Wir halten')[0]\n",
    "\n",
    "# This video is more of a compilation: (Herr Reichert, Sie haben es ja)\n",
    "aligned_df_temp.at[172,'speech_text'] = afd_speeches_cleaned.iloc[1107]['text']\n",
    "aligned_df_temp.at[172,'session_date'] = afd_speeches_cleaned.iloc[1107]['session_date']\n",
    "aligned_df_temp.at[172,'speaker'] = afd_speeches_cleaned.iloc[1107]['speaker']\n",
    "aligned_df_temp.at[172,'transcription_text'] = aligned_df_temp.at[172,'transcription_text'].split('Laut einer')[1]\n",
    "\n",
    "# This video is a discussion between the AfD MdB and the speaker (Weil Sie die Wirklichkeit nicht akzeptieren)\n",
    "aligned_df_temp.at[265,'speech_text'] = afd_speeches_cleaned.iloc[1350]['text']\n",
    "aligned_df_temp.at[265,'session_date'] = afd_speeches_cleaned.iloc[1350]['session_date']\n",
    "aligned_df_temp.at[265,'speaker'] = afd_speeches_cleaned.iloc[1350]['speaker']\n",
    "aligned_df_temp.at[265,'transcription_text'] = aligned_df_temp.at[265,'transcription_text'].split('Ihr Offenbarungsverbot')[1]\n",
    "# This video as well (Vielen Dank, Frau Präsidentin)\n",
    "aligned_df_temp.at[411,'speech_text'] = afd_speeches_cleaned.iloc[1350]['text']\n",
    "aligned_df_temp.at[411,'session_date'] = afd_speeches_cleaned.iloc[1350]['session_date']\n",
    "aligned_df_temp.at[411,'speaker'] = afd_speeches_cleaned.iloc[1350]['speaker']\n",
    "aligned_df_temp.at[411,'transcription_text'] = aligned_df_temp.at[411,'transcription_text'].split('Die Beschlussfähigkeit')[0]\n",
    "# In this video the audio is not only the speech (Herr Habeck, der bei der Nationalhymne)\n",
    "aligned_df_temp.at[281,'speech_text'] = afd_speeches_cleaned.iloc[1818]['text'] \n",
    "aligned_df_temp.at[281,'session_date'] = afd_speeches_cleaned.iloc[1818]['session_date']\n",
    "aligned_df_temp.at[281,'speaker'] = afd_speeches_cleaned.iloc[1818]['speaker']\n",
    "aligned_df_temp.at[281,'transcription_text'] = aligned_df_temp.at[281,'transcription_text'].split('Untertitelung')[0]\n",
    "# This is more of a discussion between the speaker and the visitors (Wir von der AfD erkennen diesen Völkermord) -> Sichert\n",
    "aligned_df_temp.at[227,'speech_text'] = afd_speeches_cleaned.iloc[2458]['text'] \n",
    "aligned_df_temp.at[227,'session_date'] = afd_speeches_cleaned.iloc[2458]['session_date']\n",
    "aligned_df_temp.at[227,'speaker'] = afd_speeches_cleaned.iloc[2458]['speaker']\n",
    "aligned_df_temp.at[277,'transcription_text'] = aligned_df_temp.at[277,'transcription_text'].split('Ich weise')[0]\n",
    "\n",
    "# Misses where it wasn't found, even though the speech is in the protocols (normaly because the transcription is not near enough to the protocol) for example when there are many special characters needed\n",
    "## (Im Juni 23 bringt die Union)\n",
    "aligned_df_temp.at[123,'speech_text'] = afd_speeches_cleaned.iloc[1526]['text']\n",
    "aligned_df_temp.at[123,'session_date'] = afd_speeches_cleaned.iloc[1526]['session_date']\n",
    "aligned_df_temp.at[123,'speaker'] = afd_speeches_cleaned.iloc[1526]['speaker']   \n",
    "## (Ja, dass Ihnen das nicht gefällt) -> Frohnmeier\n",
    "aligned_df_temp.at[72,'speech_text'] = afd_speeches_cleaned.iloc[2015]['text']\n",
    "aligned_df_temp.at[72,'session_date'] = afd_speeches_cleaned.iloc[2015]['session_date']\n",
    "aligned_df_temp.at[72,'speaker'] = afd_speeches_cleaned.iloc[2015]['speaker']  \n",
    "## Eigennutz, Klüngelei, Vetternwirtschaft -> Brandner\n",
    "aligned_df_temp.at[41,'speech_text'] = afd_speeches_cleaned.iloc[2091]['text']\n",
    "aligned_df_temp.at[41,'session_date'] = afd_speeches_cleaned.iloc[2091]['session_date'] \n",
    "aligned_df_temp.at[41,'speaker'] = afd_speeches_cleaned.iloc[2091]['speaker']\n",
    "## (Hinter dem Terror der letzten Generation) -> Brandner\n",
    "aligned_df_temp.at[270,'speech_text'] = afd_speeches_cleaned.iloc[2679]['text']\n",
    "aligned_df_temp.at[270,'session_date'] = afd_speeches_cleaned.iloc[2679]['session_date']\n",
    "aligned_df_temp.at[270,'speaker'] = afd_speeches_cleaned.iloc[2679]['speaker']\n",
    "## (Übermorgen ist 1. Advent) -> Bohringer \n",
    "aligned_df_temp.at[440,'speech_text'] = afd_speeches_cleaned.iloc[2612]['text']\n",
    "aligned_df_temp.at[440,'session_date'] = afd_speeches_cleaned.iloc[2612]['session_date']\n",
    "aligned_df_temp.at[440,'speaker'] = afd_speeches_cleaned.iloc[2612]['speaker']\n",
    "## (Was vielerorts und auch hier von) -> Brander\n",
    "aligned_df_temp.at[364,'speech_text'] = afd_speeches_cleaned.iloc[2679]['text']\n",
    "aligned_df_temp.at[364,'session_date'] = afd_speeches_cleaned.iloc[2679]['session_date']\n",
    "aligned_df_temp.at[364,'speaker'] = afd_speeches_cleaned.iloc[2679]['speaker']\n",
    "aligned_df_temp.at[161,'speech_text'] = afd_speeches_cleaned.iloc[2679]['text']\n",
    "aligned_df_temp.at[161,'session_date'] = afd_speeches_cleaned.iloc[2679]['session_date']\n",
    "aligned_df_temp.at[161,'speaker'] = afd_speeches_cleaned.iloc[2679]['speaker']\n",
    "## (Die Bürger haben jedes Recht, gegen) -> Weidel\n",
    "aligned_df_temp.at[142,'speech_text'] = afd_speeches_cleaned.iloc[3010]['text']\n",
    "aligned_df_temp.at[142,'session_date'] = afd_speeches_cleaned.iloc[3010]['session_date'] \n",
    "aligned_df_temp.at[142,'speaker'] = afd_speeches_cleaned.iloc[3010]['speaker']\n",
    "## (Wenn Migranten uns Deutsche) -> Baumann\n",
    "aligned_df_temp.at[195,'speech_text'] = afd_speeches_cleaned.iloc[3072]['text']\n",
    "aligned_df_temp.at[195,'session_date'] = afd_speeches_cleaned.iloc[3072]['session_date'] \n",
    "aligned_df_temp.at[195,'speaker'] = afd_speeches_cleaned.iloc[3072]['speaker']\n",
    "## (230.000 Stromsperren) -> Espendilla\n",
    "aligned_df_temp.at[429,'speech_text'] = afd_speeches_cleaned.iloc[3676]['text']\n",
    "aligned_df_temp.at[429,'session_date'] = afd_speeches_cleaned.iloc[3676]['session_date'] \n",
    "aligned_df_temp.at[429,'speaker'] = afd_speeches_cleaned.iloc[3676]['speaker']\n",
    "## Die ganze Welt setzt auf die nahezu -> Korte\n",
    "aligned_df_temp.at[95,'speech_text'] = afd_speeches_cleaned.iloc[3691]['text']\n",
    "aligned_df_temp.at[95,'session_date'] = afd_speeches_cleaned.iloc[3691]['session_date'] \n",
    "aligned_df_temp.at[95,'speaker'] = afd_speeches_cleaned.iloc[3691]['speaker']\n",
    "## Es ist unerträglich was in dieser Fraktion -> Brander\n",
    "aligned_df_temp.at[91,'speech_text'] = afd_speeches_cleaned.iloc[3697]['text']\n",
    "aligned_df_temp.at[91,'session_date'] = afd_speeches_cleaned.iloc[3697]['session_date']\n",
    "aligned_df_temp.at[91,'speaker'] = afd_speeches_cleaned.iloc[3697]['speaker']\n",
    "## Ich weiß nicht, was Sie hier eigentlich machen -> Weidel\n",
    "aligned_df_temp.at[315,'speech_text'] = afd_speeches_cleaned.iloc[14]['text']\n",
    "aligned_df_temp.at[315,'session_date'] = afd_speeches_cleaned.iloc[14]['session_date'] \n",
    "aligned_df_temp.at[315,'speaker'] = afd_speeches_cleaned.iloc[14]['speaker']\n",
    "\n",
    "# Misses where I don't understand why the speech wasn't recognized\n",
    "## (Vielen Dank, Frau Präsidentin) -> Brandner\n",
    "aligned_df_temp.at[411,'speech_text'] = \"Vielen Dank, Frau Präsidentin. – § 45 Absatz 1 der Geschäftsordnung sieht vor, dass der Bundestag beschlussfähig ist, wenn mehr als die Hälfte seiner Mitglieder anwesend ist. Der Bundestag hat zurzeit 736 Mitglieder, mehr als die Hälfte wären 369 Mitglieder. Ich habe gerade mal durchgezählt: Es dürften ungefähr 250 bis 300 Mitglieder fehlen, um die Beschlussfähigkeit des Deutschen Bundestags herzustellen. Deshalb bezweifle ich nach § 45 Absatz 2 der Geschäftsordnung für die Fraktion der Alternative für Deutschland die Beschlussfähigkeit des Bundestages. Ich weiß, dass sich das Präsidium dazu gleich verständigen wird. Ich behalte mir für den Fall, dass das Präsidium sich einig sein sollte, dass die Beschlussfähigkeit gegeben ist, vor, eine namentliche Abstimmung zu beantragen. Vielen Dank.\"\n",
    "aligned_df_temp.at[411,'session_date'] = pd.to_datetime('2023-07-07')\n",
    "aligned_df_temp.at[411,'speaker'] = \"Stephan Brandner\"\n",
    "\n",
    "## (Wie ist das vereinbar mit dem Gebot) -> Storch\n",
    "aligned_df_temp.at[6,'speech_text'] = \"Wie ist das vereinbar mit dem Gebot des Bundesverfassungsgerichtes, dass das Geschlecht eindeutig und dauerhaft sein muss – mit Blick auf das Selbstbestimmungsgesetz, das jetzt alles völlig chaotisiert?\"\n",
    "aligned_df_temp.at[6,'session_date'] = pd.to_datetime('2023-06-14')\n",
    "aligned_df_temp.at[6,'speaker'] = \"Beatrix von Storch\"\n",
    "\n",
    "## (Da die Grünen das Problem) -> Bleck\n",
    "speech_bleck = \"\"\" Werte Frau Präsidentin! Werte Kolleginnen und Kollegen! Werter Herr Kollege Träger, ich bin überhaupt nicht verwundert, vor allem, weil ich weiß, dass Sie wider besseres Wissen sprechen. Sie wissen ganz genau, dass Ihre undemokratischen Maßnahmen dazu führen, dass nicht alle AfD-Abgeordneten teilnehmen können, die gerne teilnehmen würden.\n",
    "In der Europäischen Union ist die Bundesregierung wieder einmal der Geisterfahrer. Sie lehnt die Aufnahme der Kernenergie in die Taxonomie ab, und das, obwohl sie sowohl CO2-arm als auch grundlastfähig ist. Sie ist also die Antwort auf die Frage, wie man Klimaschutz und Versorgungssicherheit in Einklang bringen kann.\n",
    "Da die Grünen das Problem der erneuerbaren Energien bei der Versorgungssicherheit nicht verstehen und sich Vizepräsidentin Katrin Göring-Eckardt Poesie im Deutschen Bundestag wünscht, versuche ich, dieses Problem in einfacher Sprache poetisch am Beispiel der Windkraft mit Wilhelm Busch zu erklären:\n",
    "Aus der Mühle schaut der Müller, Der so gerne mahlen will. Stiller wird der Wind und stiller, Und die Mühle stehet still.\n",
    "So gehts immer, wie ich finde, Rief der Müller voller Zorn. Hat man Korn, so fehlts am Winde. Hat man Wind, so fehlt das Korn.\n",
    "Ja, werte Kolleginnen und Kollegen, im Unterschied zum 21. Jahrhundert wusste man im 19. Jahrhundert, dass auf die Windkraft nicht ohne Weiteres Verlass ist.\n",
    "In Deutschland interessiert sich die Bundesregierung zwar für Klimatreiber, nicht aber für Preistreiber. Die Folge: 4 Prozent Inflation und explodierende Strom- und Gaspreise. Mit der EEG-Umlage und der CO2-Abgabe werden die Bürger gnadenlos abkassiert. Die Regierungen von Polen und Tschechien wollen ihre Bürger mit einer Aufhebung oder Senkung der Mehrwert- bzw. Umsatzsteuer auf Strom und Gas entlasten. Und die Bundesregierung? Bundeslandwirtschaftsminister Cem Özdemir sinniert währenddessen über höhere Lebensmittelpreise. Herzlichen Glückwunsch! Ihre Politik gegen die globale Erwärmung ist eine Politik der sozialen Kälte.\n",
    "Darüber hinaus positioniert sich die Bundesregierung im Spannungsfeld zwischen Klimaschutz und Artenschutz völlig einseitig. Der Ausbau der erneuerbaren Energien soll im öffentlichen Interesse sein und der öffentlichen Sicherheit dienen. Habeck nennt das: die Energiewende mit Artenschutz versöhnen. Er verwechselt offenbar „versöhnen“ und „versündigen“.\n",
    "Fakt ist: Windkraftanlagen töten jährlich Hunderttausende Vögel und Fledermäuse. Eine Fläche, die etwa dreimal so groß wie das Saarland ist, wollen Sie mit Windkraftanlagen verspargeln. Damit opfern insbesondere die grünen Klimaapostel den Artenschutz auf dem Altar der Energiewende.\n",
    "Und Widerspruch aus dem Bundesumweltministerium gibt es nicht.\n",
    "Die Pläne der Bundesregierung zum Ausbau der erneuerbaren Energien sind gefährlich. Windkraftanlagen sollen näher an Häuser gebaut werden. Dadurch werden Bewohner durch Infraschall stärker gesundheitlich belastet. Windkraftanlagen sollen auch näher an Drehfunkfeuer gebaut werden. Dadurch werden die Signale zur Orientierung von Flugzeugen stärker gestört.\n",
    "Sie, werte Kolleginnen und Kollegen, ignorieren das öffentliche Interesse. Sie sind ein Sicherheitsrisiko.\n",
    "Es ist unfassbar, mit welcher Dreistigkeit Sie die Wirklichkeit verdrehen.\n",
    "Doch die größte Enttäuschung der Ampelkoalition – von Ihnen habe ich nichts anderes erwartet – ist tatsächlich die FDP. Früher forderte sie unter anderem die Abschaffung der EEG-Umlage, ein Verbot des Baus von Windkraftanlagen in Wäldern und eine technologieoffene Förderung. Das hat sie ja mit gutem Grund gefordert. Heute ist davon aber nichts mehr übrig.\n",
    "Für die FDP waren diese Forderungen bei der Bildung der Ampelkoalition Verhandlungsmasse, die man jeweils für vier Ministersitze und Ministerwagen bereitwillig aufgegeben hat.\n",
    "Damit nimmt sich die Ampelkoalition tatsächlich eine Ampel zum Vorbild: Bei einer Ampel sieht man häufig Rot und Grün, und bei Gelb hält sowieso niemand.\n",
    "Vielen Dank.\"\"\"\n",
    "aligned_df_temp.at[10,'speech_text'] = speech_bleck\n",
    "aligned_df_temp.at[271, 'speech_text'] = speech_bleck\n",
    "aligned_df_temp.at[10,'session_date'] = pd.to_datetime('2022-01-12')\n",
    "aligned_df_temp.at[271,'session_date'] = pd.to_datetime('2022-01-12')\n",
    "aligned_df_temp.at[10,'speaker'] = \"Andreas Bleck\"\n",
    "aligned_df_temp.at[271,'speaker'] = \"Andreas Bleck\"\n",
    "\n",
    "# The following speeches were uploaded during the 20. legislative period, but are from the 19.as_integer_ratio\n",
    "aligned_df_temp.drop(index = 432, inplace = True) # Von rund 4.000 bislang\n",
    "aligned_df_temp.drop(index = 347, inplace = True) # Was unsere Regierung -> Corta\n",
    "aligned_df_temp.drop(index = 385, inplace = True) # Ihr Schwarz-Weiß-Denken -> Chrupalla\n",
    "aligned_df_temp.drop(index = 408, inplace = True) # Nun wollen Sie sich -> Brandner\n",
    "aligned_df_temp.drop(index = 436, inplace = True) # Nach dem Vorbild des schädlichen -> Curio\n",
    "aligned_df_temp.drop(index = 439, inplace = True) # Aus unserer Sicht wäre es besser -> Frömming\n",
    "aligned_df_temp.drop(index = 425, inplace = True) # Denn es geht uns als Parlamentarier -> Huber\n",
    "aligned_df_temp.drop(index = 392, inplace = True) # Wir müssen den Irrweg so schnell -> Sichert\n",
    "aligned_df_temp.drop(index = 360, inplace = True) \n",
    "aligned_df_temp.drop(index = 383, inplace = True)\n",
    "aligned_df_temp.drop(index = 379, inplace = True) \n",
    "aligned_df_temp.drop(index = 370, inplace = True) # Natürlich kann man alle Transaktinen\n",
    "aligned_df_temp.drop(index = 371, inplace = True) # Dann muss man sich schon fragen,\n",
    "aligned_df_temp.drop(index = 428, inplace = True) \n",
    "\n",
    "\n",
    "finished_alignment_df = aligned_df_temp\n",
    "\n",
    "finished_alignment_df = spacy_pipeline(cleaned_speeches = finished_alignment_df, text_column='speech_text')\n",
    "finished_alignment_df.to_pickle(\"data/preprocessed_data/finished_alignment_df.pkl\")\n",
    "finished_alignment_df.to_csv(\"data/preprocessed_data/finished_alignment_df.csv\")\n",
    "print(\"Finished alignment and cleanup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Analysis\n",
    "\n",
    "In the following the data is analyzed according to the theoretically deducted hypothesis. The main dataframes used for this are the finished_alignement and afd_speeches_cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Getting to know the data set\n",
    "\n",
    "The following analysis are not part of testing any hypotheses but just to visualize some interesting aspects of the data set including:\n",
    "- How long are the Videos (in words)?\n",
    "- How are the uploads distributed over time and platform?\n",
    "- How far do giving the speech and it being uploaded lay apart?\n",
    "- Are the same videos uploaded on different platforms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"data/preprocessed_data/finished_alignment_df.pkl\"):\n",
    "    aligned_df = pd.read_pickle(\"data/preprocessed_data/finished_alignment_df.pkl\")\n",
    "elif os.path.exists(\"data/preprocessed_data/finished_alignment_df.csv\"):\n",
    "    aligned_df = pd.read_csv(\"data/preprocessed_data/finished_alignment_df.csv\")\n",
    "else:\n",
    "    print(\"No data found. Please run the alignement code block first.\")\n",
    "\n",
    "if os.path.exists(\"data/preprocessed_data/afd_speeches_cleaned.pkl\"):\n",
    "    afd_speeches_cleaned_df = pd.read_pickle(\"data/preprocessed_data/afd_speeches_cleaned.pkl\")\n",
    "elif os.path.exists(\"data/preprocessed_data/afd_speeches_cleaned.csv\"):\n",
    "    afd_speeches_cleaned_df = pd.read_csv(\"data/preprocessed_data/afd_speeches_cleaned.csv\")\n",
    "else:\n",
    "    print(\"No data found. Please run the alignement code block first.\")\n",
    "\n",
    "\n",
    "merged_aligned_df = pd.merge(aligned_df, afd_speeches_cleaned_df, left_on = 'speech_text', right_on = 'text', how = 'left')\n",
    "\n",
    "merged_aligned_df.to_pickle(\"data/preprocessed_data/merged_aligned_df.pkl\")\n",
    "merged_aligned_df.to_csv(\"data/preprocessed_data/merged_aligned_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\"\n",
    "How long are the videos in words?\n",
    " x: Number of videos, \n",
    " y: length of the video in words, \n",
    " extra text for average and median\n",
    "\"\"\"\n",
    "merged_aligned_df['video_length'] = merged_aligned_df['tokens_x'].apply(len)\n",
    "average_video_length = merged_aligned_df['video_length'].mean()\n",
    "print(\"Average video length: \", average_video_length)\n",
    "media_video_length = merged_aligned_df['video_length'].median()\n",
    "print(\"Median video length: \", media_video_length)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "palette = sns.color_palette('pastel')\n",
    "sns.histplot(merged_aligned_df['video_length'], bins=100, kde=False, color=palette[0])\n",
    "\n",
    "plt.xlabel('Video Length (in words)', fontsize=12)\n",
    "plt.ylabel('Number of Videos', fontsize=12)\n",
    "plt.title('Distribution of Video Lengths', fontsize=14)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "How are the uploades distributed over time and platforms?\n",
    " x: number of videos, differentiated by platform \n",
    " y: date of upload, \n",
    " extra text for containing the total number of uploads per platform\"\n",
    "\"\"\"\n",
    "merged_aligned_df['instagram'] = merged_aligned_df['uploaded_source'].apply(lambda x: 1 if 'Insta' in x else 0)\n",
    "merged_aligned_df['tiktok'] = merged_aligned_df['uploaded_source'].apply(lambda x: 1 if 'TikTok' in x else 0)\n",
    "merged_aligned_df['youtube'] = merged_aligned_df['uploaded_source'].apply(lambda x: 1 if 'AfD-Fraktion' in x or 'AfD-TV' in x else 0)\n",
    "\n",
    "total_instagram = merged_aligned_df['instagram'].sum()\n",
    "total_tiktok = merged_aligned_df['tiktok'].sum()\n",
    "total_youtube = merged_aligned_df['youtube'].sum()\n",
    "\n",
    "# Ensure the 'date' column is in datetime format\n",
    "merged_aligned_df['transcription_date'] = pd.to_datetime(merged_aligned_df['transcription_date'])\n",
    "\n",
    "# Extract the year and month for grouping\n",
    "merged_aligned_df['year_month'] = merged_aligned_df['transcription_date'].dt.to_period('M')\n",
    "\n",
    "# Group by year and month, summing the counts for each platform\n",
    "monthly_counts = merged_aligned_df.groupby('year_month')[['instagram', 'tiktok', 'youtube']].sum()\n",
    "\n",
    "# Convert the index back to datetime for plotting\n",
    "monthly_counts.index = monthly_counts.index.to_timestamp()\n",
    "\n",
    "# Create the stacked bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(monthly_counts.index))  # X positions for the bars\n",
    "\n",
    "palette = sns.color_palette('pastel')\n",
    "plt.bar(x, monthly_counts['instagram'], label='Instagram', color=palette[0])\n",
    "plt.bar(x, monthly_counts['tiktok'], bottom=monthly_counts['instagram'], label='TikTok', color=palette[1])\n",
    "plt.bar(x, monthly_counts['youtube'], bottom=monthly_counts['instagram'] + monthly_counts['tiktok'], label='YouTube', color=palette[2])\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Number of Videos', fontsize=12)\n",
    "plt.title('Number of Videos Uploaded Each Month by Platform', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "# Adjust x-axis ticks\n",
    "plt.xticks(x, monthly_counts.index.strftime('%Y-%m'), rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How far do giving speeches and uploading videos lay apart?\n",
    "    x: number of speeches, \n",
    "    y: time between speech and upload, \n",
    "    extra text for longest time and average time\n",
    "\"\"\"\n",
    "\n",
    "merged_aligned_df['time_diff'] = merged_aligned_df['transcription_date'] - merged_aligned_df['session_date_x']\n",
    "merged_aligned_df['time_diff'] = merged_aligned_df['time_diff'].dt.total_seconds() / 3600 / 24  # Convert to days\n",
    "\n",
    "average_time_diff = merged_aligned_df['time_diff'].mean()\n",
    "median_time_diff = merged_aligned_df['time_diff'].median()\n",
    "longest_time_diff = merged_aligned_df['time_diff'].max()    \n",
    "\n",
    "print(\"Average time difference: \", average_time_diff)\n",
    "print(\"Median time difference: \", median_time_diff)\n",
    "print(\"Longest time difference: \", longest_time_diff, \"with upload date: \", merged_aligned_df.loc[merged_aligned_df['time_diff'].idxmax(), 'transcription_date'], \"and speech date: \", merged_aligned_df.loc[merged_aligned_df['time_diff'].idxmax(), 'session_date_x'])\n",
    "\n",
    "merged_aligned_df['year_month'] = merged_aligned_df['session_date_x'].dt.to_period('M')\n",
    "monthly_avg_time_diff = merged_aligned_df.groupby('year_month')['time_diff'].mean().reset_index()\n",
    "monthly_avg_time_diff['year_month'] = monthly_avg_time_diff['year_month'].dt.to_timestamp()\n",
    "\n",
    "palette = sns.color_palette('pastel')\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=monthly_avg_time_diff,\n",
    "    x='year_month',\n",
    "    y='time_diff',\n",
    "    color=palette[0],\n",
    ")\n",
    "\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Average Time Difference (days)', fontsize=12)\n",
    "plt.title('Average Time Difference Between Speech and Upload by Month', fontsize=14)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Are the same videos uploaded on different platforms?\n",
    "\n",
    "Grid with uploads to each platform\n",
    "\"\"\"\n",
    "merged_aligned_df['faction'] = merged_aligned_df['uploaded_source'].apply(lambda x: 1 if 'Faction' in x or 'Fraktion' in x or 'TikTok' in x else 0) # Every video uploaded to TikTok is by the faction as the party was banned\n",
    "merged_aligned_df['party'] = merged_aligned_df['uploaded_source'].apply(lambda x: 1 if 'Party' in x else 0)\n",
    "\n",
    "num_faction = merged_aligned_df['faction'].sum(); print(\"Number of faction videos: \", num_faction)\n",
    "num_party = merged_aligned_df['party'].sum(); print(\"Number of party videos: \", num_party)\n",
    "num_both = merged_aligned_df[(merged_aligned_df['faction'] == 1) & (merged_aligned_df['party'] == 1)].shape[0]; print(\"Number of videos uploaded by both: \", num_both)\n",
    "\n",
    "\n",
    "num_tik_tok = merged_aligned_df['tiktok'].sum(); print(\"Number of TikTok videos: \", num_tik_tok)\n",
    "num_instagram = merged_aligned_df['instagram'].sum(); print(\"Number of Instagram videos: \", num_instagram)\n",
    "num_youtube = merged_aligned_df['youtube'].sum();  print(\"Number of YouTube videos: \", num_youtube)\n",
    "num_only_tik_tok = merged_aligned_df[(merged_aligned_df['tiktok'] == 1) & (merged_aligned_df['instagram'] == 0) & (merged_aligned_df['youtube'] == 0)].shape[0]; print(\"Number of TikTok videos only: \", num_only_tik_tok)\n",
    "num_only_instagram = merged_aligned_df[(merged_aligned_df['instagram'] == 1) & (merged_aligned_df['tiktok'] == 0) & (merged_aligned_df['youtube'] == 0)].shape[0]; print(\"Number of Instagram videos only: \", num_only_instagram)\n",
    "num_only_youtube = merged_aligned_df[(merged_aligned_df['youtube'] == 1) & (merged_aligned_df['instagram'] == 0) & (merged_aligned_df['tiktok'] == 0)].shape[0]; print(\"Number of YouTube videos only: \", num_only_youtube)\n",
    "num_tik_tok_instagram = merged_aligned_df[(merged_aligned_df['instagram'] == 1) & (merged_aligned_df['tiktok'] == 1) & (merged_aligned_df['youtube'] == 0)].shape[0]; print(\"Number of TikTok and Instagram videos: \", num_tik_tok_instagram)\n",
    "num_tik_tok_youtube = merged_aligned_df[(merged_aligned_df['youtube'] == 1) & (merged_aligned_df['tiktok'] == 1) & (merged_aligned_df['instagram'] == 0)].shape[0]; print(\"Number of TikTok and YouTube videos: \", num_tik_tok_youtube)\n",
    "num_instagram_youtube = merged_aligned_df[(merged_aligned_df['youtube'] == 1) & (merged_aligned_df['instagram'] == 1) & (merged_aligned_df['tiktok'] == 0)].shape[0]; print(\"Number of Instagram and YouTube videos: \", num_instagram_youtube)\n",
    "num_all_three = merged_aligned_df[(merged_aligned_df['youtube'] == 1) & (merged_aligned_df['instagram'] == 1) & (merged_aligned_df['tiktok'] == 1)].shape[0]; print(\"Number of videos on all three platforms: \", num_all_three)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Meta-Data of the speeches\n",
    "\n",
    "The following analysis test the hypotheses \n",
    "- <b> 1.1 </b> Speeches given by a prominent figure in the party are overrepresented.\n",
    "- <b> 1.2 </b> Speeches given during a agenda item, which is not debate about a bill, are overrepresented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Speeches given by a prominent figute in the party are overrepresented\n",
    "# This is tested by just counting the number of speeches any given person holds and comparing them (by hand) to the party and faction leadership\n",
    "\n",
    "if os.path.exists(\"data/preprocessed_data/merged_aligned_df.pkl\"):\n",
    "    merged_aligned_df = pd.read_pickle(\"data/preprocessed_data/merged_aligned_df.pkl\")\n",
    "elif os.path.exists(\"data/preprocessed_data/merged_aligned_df.csv\"):\n",
    "    merged_aligned_df = pd.read_csv(\"data/preprocessed_data/merged_aligned_df.csv\")\n",
    "else:\n",
    "    print(\"No data found. Please run the alignement code block first.\")\n",
    "\n",
    "if os.path.exists(\"data/preprocessed_data/afd_speeches_cleaned.pkl\"):\n",
    "    afd_speeches_cleaned_df = pd.read_pickle(\"data/preprocessed_data/afd_speeches_cleaned.pkl\")\n",
    "elif os.path.exists(\"data/preprocessed_data/afd_speeches_cleaned.csv\"):\n",
    "    afd_speeches_cleaned_df = pd.read_csv(\"data/preprocessed_data/afd_speeches_cleaned.csv\")\n",
    "else:\n",
    "    print(\"No data found. Please run the alignement code block first.\")\n",
    "\n",
    "\"\"\"\n",
    "Counts all speeches given by each speaker and the uploaded speeches given by each speaker\n",
    "and returns a dataframe with the counts.\n",
    "@Input:\n",
    "    all_speeches_df: The dataframe with all speeches\n",
    "    uploaded_speeches_df: The dataframe with all uploaded speeches\n",
    "    speaker_column: The column with the speaker names\n",
    "@Output:\n",
    "    A dataframe with the counts of all speeches given by each speaker and the uploaded speeches given by each speaker\n",
    "\"\"\"\n",
    "def count_speeches_by_speaker(all_speeches_df, uploaded_speeches_df, speaker_column_all, speaker_column_up) -> pd.DataFrame:\n",
    "    # Count the number of speeches by each speaker\n",
    "    speech_counts_all = all_speeches_df[speaker_column_all].value_counts().reset_index()\n",
    "    speech_counts_all.columns = ['speaker', 'speech_count']\n",
    "    speech_counts_all['percentage_all'] = speech_counts_all['speech_count'] / speech_counts_all['speech_count'].sum() * 100\n",
    "\n",
    "    speech_counts_up = uploaded_speeches_df[speaker_column_up].value_counts().reset_index()\n",
    "    speech_counts_up.columns = ['speaker', 'upload_count']\n",
    "    speech_counts_up['percentage_up'] = speech_counts_up['upload_count'] / speech_counts_up['upload_count'].sum() * 100\n",
    "    # Merge the two dataframes on the speaker column\n",
    "    speech_counts = pd.merge(speech_counts_all, speech_counts_up, on='speaker', how='left')\n",
    "    return speech_counts\n",
    "\n",
    "speech_counts = count_speeches_by_speaker(afd_speeches_cleaned_df, merged_aligned_df, 'speaker', 'speaker_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results via a bar plot according to the percentage of uploaded speeches using seaborn\n",
    "speech_counts.sort_values(by='percentage_up', ascending=False, inplace=True)\n",
    "speech_counts = speech_counts[speech_counts['percentage_up'] >= 1]\n",
    "speech_counts['percentage_up'] = speech_counts['percentage_up'].round(1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_plot = sns.barplot(\n",
    "    data=speech_counts, \n",
    "    x='percentage_up', \n",
    "    y='speaker',\n",
    "    dodge=False,\n",
    "    hue = 'speaker'\n",
    ")\n",
    "\n",
    "# Ensure the number of labels matches the number of bars\n",
    "labels = [\n",
    "    f\"{row['percentage_up']} ({round(row['percentage_all'],1)})\"\n",
    "    for _, row in speech_counts.iterrows()\n",
    "]\n",
    "\n",
    "for container, label in zip(bar_plot.containers, labels):\n",
    "    bar_plot.bar_label(container, labels=[label] * len(container.datavalues), label_type='edge')\n",
    "\n",
    "\n",
    "bar_plot.set_title('Percentage of Uploaded Speeches by Speaker with percentage of all speeches in paranthesis', fontsize=16)\n",
    "bar_plot.set_xlabel('Percentage of Uploaded Speeches', fontsize=14)\n",
    "bar_plot.set_ylabel('Speaker', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged speeches with transcriptions and saved to file.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"data/preprocessed_data/merged_aligned_df.pkl\"):\n",
    "    merged_aligned_df = pd.read_pickle(\"data/preprocessed_data/merged_aligned_df.pkl\")\n",
    "elif os.path.exists(\"data/preprocessed_data/merged_aligned_df.csv\"):\n",
    "    merged_aligned_df = pd.read_csv(\"data/preprocessed_data/merged_aligned_df.csv\")\n",
    "else:\n",
    "    print(\"No data found. Please run the alignement code block first.\")\n",
    "\n",
    "if os.path.exists(\"data/preprocessed_data/afd_speeches_cleaned.pkl\"):\n",
    "    afd_speeches_cleaned_df = pd.read_pickle(\"data/preprocessed_data/afd_speeches_cleaned.pkl\")\n",
    "elif os.path.exists(\"data/preprocessed_data/afd_speeches_cleaned.csv\"):\n",
    "    afd_speeches_cleaned_df = pd.read_csv(\"data/preprocessed_data/afd_speeches_cleaned.csv\")\n",
    "else:\n",
    "    print(\"No data found. Please run the alignement code block first.\")\n",
    "\n",
    "# Merge the two dataframes on the text column\n",
    "speeches_with_transcriptions_df = pd.merge(afd_speeches_cleaned_df, merged_aligned_df, left_on = 'text', right_on = 'speech_text', how = 'left')\n",
    "del afd_speeches_cleaned_df\n",
    "del merged_aligned_df\n",
    "speeches_with_transcriptions_df = speeches_with_transcriptions_df.filter(['speaker', 'session_x', 'session_date', 'text_x', 'transcription_text', 'agenda_item_x', 'doc', 'transcription_text', 'transcription_date', 'uploaded_source'])\n",
    "\n",
    "# Save the merged dataframe\n",
    "speeches_with_transcriptions_df.to_pickle(\"data/speeches_with_transcriptions_df.pkl\") #-> This is a big file of more than 1GB\n",
    "speeches_with_transcriptions_df.to_csv(\"data/speeches_with_transcriptions_df.csv\")\n",
    "print(\"Merged speeches with transcriptions and saved to file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Speeches given during an agenda item, which is not a debate about a bill, are overrepresented\n",
    "# This is done by just counting the number of speeches given during any type of agenda item and comparing them (by hand) \n",
    "# -> this is a bit messy, as the protocols are not always clear about the agenda item\n",
    "\n",
    "def count_speeches_by_agenda_item(speeches_with_transcriptions_df) -> pd.DataFrame:\n",
    "    # Count the number of speeches by each agenda item\n",
    "    bill_debate_searchstrings = ['Enturf', 'Gesetz', 'Gesetzes', 'Änderung', 'Erste', 'Zweite', 'Dritte', 'Lesung', 'Beratung des Antrages', 'Beratung des Antrags', 'Gesetzentwurf', 'Gesetzentwürfe', 'Ausschuss', 'Einzelplan']\n",
    "    non_bill_debate_searchstrings = ['Aktuelle Stunde', 'Anfrage', 'Befragung', 'Bundesregierung', 'Bundeskanzler', 'Erklärung', 'Fragestunde', 'Geschäftsordnung', 'Wahlvorschlag', 'Eröffne die Wahlen', 'Vereinbarte Debatte', 'Beratung der Unterrichtung', 'Wahlvorschläge', 'Ältestenrat', 'Schriftführer']\n",
    "    speeches_with_transcriptions_df['item_all'] = speeches_with_transcriptions_df['agenda_item_x'].apply(lambda x: 'bill' if any(searchstring.lower() in x.lower() for searchstring in bill_debate_searchstrings) else 'non_bill' if any(searchstring.lower() in x.lower() for searchstring in non_bill_debate_searchstrings) else 'other')\n",
    "    speeches_with_transcriptions_df['afd_bill'] = speeches_with_transcriptions_df.apply(lambda row: 1 if row['item_all'] == 'bill' and 'afd' in row['agenda_item_x'].lower() else 0,axis=1)\n",
    "    return speeches_with_transcriptions_df\n",
    "items = count_speeches_by_agenda_item(speeches_with_transcriptions_df).filter(['agenda_item_x', 'agenda_item_y', 'item_all','afd_bill','speaker', 'session_date', 'transcription_date', 'uploaded_source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['item_up'] = items['item_all'].where(items['uploaded_source'].notnull())\n",
    "items['afd_bill_up'] = items['afd_bill'].where(items['uploaded_source'].notnull())\n",
    "\n",
    "print(items['item_all'].value_counts())\n",
    "print(\"Therof bills by the AfD: \", items['afd_bill'].sum())\n",
    "print(items['item_up'].value_counts())\n",
    "\n",
    "all_bills = items['item_all'].value_counts().get('bill', 0)\n",
    "all_non_bills = items['item_all'].value_counts().get('non_bill', 0)\n",
    "all_other = items['item_all'].value_counts().get('other', 0)\n",
    "all_afd_bills = items['afd_bill'].sum()\n",
    "sum_all = all_bills + all_non_bills + all_other\n",
    "\n",
    "up_bills = items['item_up'].value_counts().get('bill', 0)\n",
    "up_non_bills = items['item_up'].value_counts().get('non_bill', 0)\n",
    "up_other = items['item_up'].value_counts().get('other', 0)\n",
    "up_afd_bills = items['afd_bill_up'].sum()\n",
    "sum_up = up_bills + up_non_bills + up_other\n",
    "\n",
    "\n",
    "print(\"Percentage of debates about bills in all speeches: \", all_bills / sum_all * 100)\n",
    "print(\"Percentage of non-bill debates in all speeches: \", all_non_bills / sum_all * 100)\n",
    "print(\"Percentage of other debates in all speeches: \", all_other / sum_all * 100)\n",
    "print(\"\\n\")\n",
    "print(\"Percentage of debates about bills in uploaded speeches: \", up_bills / sum_all * 100)\n",
    "print(\"Percentage of non-bill debates in uploaded speeches: \", up_non_bills / sum_all * 100)\n",
    "print(\"Percentage of other debates in uploaded speeches: \", up_other / sum_all * 100)\n",
    "print(\"\\n\")\n",
    "# No real effect observable\n",
    "# Maybe when differentiating between bills by the AfD and other parties\n",
    "print(\"AfD bills in uploaded speeches \", up_afd_bills, \"which is \", (up_afd_bills / up_bills) * 100, \"percent of all uploaded speeches about bills\")\n",
    "print(\"\\n\")\n",
    "print(\"Number of bills in uploaded speeches, without AfD bills:\", up_bills - up_afd_bills)\n",
    "print(\"Number of AfD bills in uploaded speeches: \", up_afd_bills)\n",
    "print(\"Number of non-bill debates in uploaded speeches: \", up_non_bills)\n",
    "print(\"Number of other debates in uploaded speeches: \", up_other)\n",
    "print(\"\\n\")\n",
    "print(\"Percentage of debates about non-afd bills in uploaded speeches: \", (up_bills - up_afd_bills) / sum_up * 100)\n",
    "print(\"Percentage of debates about afd bills in uploaded speeches: \", up_afd_bills / sum_up * 100)\n",
    "print(\"Percentage of debates about non-bill debates in uploaded speeches: \", up_non_bills / sum_up * 100)\n",
    "print(\"Percentage of debates about other in uploaded speeches: \", up_other / sum_up * 100)\n",
    "print(\"\\n\")\n",
    "print(\"Percentage of debates about non-afd bills in all speeches: \", (all_bills - all_afd_bills) / sum_all * 100)\n",
    "print(\"Percentage of debates about afd bills in all speeches: \", all_afd_bills / sum_all * 100)\n",
    "print(\"Percentage of debates about non-bill debates in all speeches: \", all_non_bills / sum_all * 100)\n",
    "print(\"Percentage of debates about other in all speeches: \", all_other / sum_all * 100)\n",
    "# Bit of an effect oberservable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Thematics and Semantics\n",
    "\n",
    "The following analysis test the hypotheses\n",
    "\n",
    "- <b> 2.1 </b> Speeches with at the time salient topics are overrepresented.\n",
    "- <b> 2.2 </b> Speeches with topics from the programmatic core of the AfDs ideology are overrepresented.\n",
    "- <b> 2.3 </b> Patterns from the standard repertoire of right wing populism are overrepresented.\n",
    "    - <b> 2.3.1 </b> Blaming plays a big role\n",
    "    - <b> 2.3.2 </b> Crisis rhetoric plays a big role\n",
    "    - <b> 2.3.3 </b> \"Us vs. them\" arguments play a big role\n",
    "    - <b> 2.3.4 </b> \"Common sense\" arguments play a big role\n",
    "    - <b> 2.3.5 </b> \"The people\" play a big role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Topics \n",
    "The following code block extracts the topics from each speech (uploaded or not). It therefor serves to answer the hypothesis 2.1 and 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION: THIS TAKES A LONG TIME TO RUN, EVEN WITH GPU SUPPORT AND 16GB OF UNIFIED MEMORY IT TOOK AROUND 30 MINUTES\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Get a list of sentences from the whole speeches and the uploaded speeches\n",
    "speeches_with_transcriptions_df['sentence_list'] = speeches_with_transcriptions_df['doc'].apply(lambda x: list(x.sents))\n",
    "speeches_with_transcriptions_df['sentence_list_up'] = speeches_with_transcriptions_df['transcription_text'].fillna(\"\").apply(lambda x: list(nlp(x).sents))\n",
    "# Filter the dataframe to only include the relevant columns for the hypothesis regarding thematics\n",
    "thematics_df = speeches_with_transcriptions_df.filter(['speaker', 'text_x', 'session_date', 'doc', 'speech_text', 'transcription_text', ' uploaded_source', 'sentence_list', 'sentence_list_up',])\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS is available\")\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    pipeline_classification = pipeline(\"text-classification\", model=\"chkla/parlbert-topic-german\", top_k=5, device=mps_device, truncation=True)\n",
    "else: \n",
    "    print(\"MPS is not available\")\n",
    "    pipeline_classification = pipeline(\"text-classification\", model=\"chkla/parlbert-topic-german\", top_k=5, device=0, truncation=True)\n",
    "\n",
    "thematics_df['text_x_score'] = thematics_df['sentence_list'].apply(lambda x: [pipeline_classification(sents.text) for sents in x])\n",
    "thematics_df['speech_score'] = thematics_df['sentence_list_up'].apply(lambda x: [pipeline_classification(sents.text) for sents in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thematics_df_pkl = thematics_df.copy()\n",
    "\n",
    "\"\"\"\n",
    "Returns a list of the lengths of a list of lists\n",
    "@Input:\n",
    "    sentences: A list of lists of sentences\n",
    "@Output:\n",
    "    A list of the lengths of the sentences in the list\n",
    "\"\"\"\n",
    "def give_lengths(sentences: list) -> list:\n",
    "    return [len(str(sentence)) if len(str(sentence)) > 0 else 0 for sentence in sentences]\n",
    "\n",
    "thematics_df_pkl['sentence_list_lengths'] = thematics_df_pkl['sentence_list'].apply(give_lengths)\n",
    "thematics_df_pkl['sentence_list_lengths_up'] = thematics_df_pkl['sentence_list_up'].apply(give_lengths)\n",
    "\n",
    "\"\"\"\n",
    "This method combines the scores of the sentences by adding them up (weighted by the length of their corresponding sentence)\n",
    "\n",
    "@Input:\n",
    "    scores_df: the dataframe\n",
    "    score_column: The column with the list of dictionaries with the scores for each sentence\n",
    "    sentence_list_column: The column with the list of sentences to extract the length of the sentences\n",
    "\n",
    "@Output:\n",
    "    A changed dataframe with a new column with the combined scores\n",
    "\"\"\"\n",
    "\n",
    "def combine_scores(scores_df, score_column, sentence_list_length_column):\n",
    "    def calculate_combined_scores(scores, lengths):\n",
    "        combined_scores = {}\n",
    "        for sentence_scores, length in zip(scores, lengths):\n",
    "            for score_entry in sentence_scores:\n",
    "                for score in score_entry:\n",
    "                    label = score['label']\n",
    "                    score_value = score['score']\n",
    "                    if label not in combined_scores:\n",
    "                        combined_scores[label] = 0\n",
    "                    combined_scores[label] += score_value * length\n",
    "        return combined_scores\n",
    "\n",
    "    # Apply the calculation to each row\n",
    "    scores_df['combined_scores_' + score_column] = scores_df.apply(\n",
    "        lambda row: calculate_combined_scores(row[score_column], row[sentence_list_length_column]), axis=1\n",
    "    )\n",
    "\n",
    "combine_scores(thematics_df_pkl, 'text_x_score', 'sentence_list_lengths')\n",
    "combine_scores(thematics_df_pkl, 'speech_score', 'sentence_list_lengths_up')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "\n",
    "# Without government score -> I got the hint, that the model likes to give \"government\" a high score, so I just remove it\n",
    "thematics_df_pkl['combined_scores_text_x_wo_gov'] = thematics_df_pkl['combined_scores_text_x_score'].apply(lambda x: {k: v for k, v in x.items() if k != 'Government'})\n",
    "thematics_df_pkl['combined_scores_speech_wo_gov'] = thematics_df_pkl['combined_scores_speech_score'].apply(lambda x: {k: v for k, v in x.items() if k != 'Government'})\n",
    "\n",
    "thematics_df_pkl['most_probable_topic_text_x'] = thematics_df_pkl['combined_scores_text_x_score'].apply(lambda x: max(x, key=x.get))\n",
    "thematics_df_pkl['most_probable_topic_speech'] = thematics_df_pkl['combined_scores_speech_score'].apply(lambda x: max(x, key=x.get, default=None))\n",
    "thematics_df_pkl['most_probable_topic_text_x_wo_gov'] = thematics_df_pkl['combined_scores_text_x_wo_gov'].apply(lambda x: max(x, key=x.get))\n",
    "thematics_df_pkl['most_probable_topic_speech_wo_gov'] = thematics_df_pkl['combined_scores_speech_wo_gov'].apply(lambda x: max(x, key=x.get, default=None))\n",
    "\n",
    "thematics_df_for_visualization = thematics_df_pkl.filter(['speaker', 'text_x', 'session_date', 'speech_text', 'transcription_text', ' uploaded_source','combined_scores_text_x_score','combined_scores_speech_score','combined_scores_text_x_wo_gov','combined_scores_speech_wo_gov','most_probable_topic_text_x','most_probable_topic_speech','most_probable_topic_text_x_wo_gov','most_probable_topic_speech_wo_gov'])\n",
    "thematics_df_for_visualization.to_pickle(\"data/thematics_df_for_visualization.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results via a seaborn lineplot, x axis is aggregated dates of the speeches, \n",
    "# y axis are the number of the most probable topic per month, differentiated by topic and color of the line\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if os.path.exists(\"data/thematics_df_for_visualization.pkl\"):\n",
    "    thematics_df_for_visualization = pd.read_pickle(\"data/thematics_df_for_visualization.pkl\")\n",
    "else: \n",
    "    print(\"No data found. Please run the alignement code block first.\")\n",
    "\n",
    "\"\"\"\n",
    "This method plots the number of times each topic was discussed per quarter, differentiated by topic and color of the line\n",
    "The underlying data is the dataframe with the combined scores of the sentences (thematics_df_for_visualization)\n",
    "\n",
    "@Input:\n",
    "    topic_column: The column with the topic names -> choose between all speeches and uploaded speeches and between with government / without government\n",
    "    excluded_topics: A list of topics to exclude from the plot -> This is sensible as some are pretty rare and not of interest most probably\n",
    "\"\"\"\n",
    "def plot_topic_counts(topic_column, excluded_topics, titel):\n",
    "    # Ensure the 'date' column is in datetime format\n",
    "    thematics_df_for_visualization['session_date'] = pd.to_datetime(thematics_df_for_visualization['session_date'])\n",
    "    thematics_df_for_visualization['year_month'] = thematics_df_for_visualization['session_date'].dt.to_period('Q')\n",
    "\n",
    "    # Count the occurrences of each topic per month\n",
    "    topic_counts = thematics_df_for_visualization.groupby(['year_month', topic_column]).size().reset_index(name='count')\n",
    "\n",
    "    # Convert 'year_month' back to datetime for plotting\n",
    "    topic_counts['year_month'] = topic_counts['year_month'].dt.to_timestamp()\n",
    "\n",
    "    filtered_topic_counts = topic_counts[~topic_counts[topic_column].isin(excluded_topics)]\n",
    "\n",
    "    # Create the line plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(\n",
    "        data=filtered_topic_counts,\n",
    "        x='year_month',\n",
    "        y='count',\n",
    "        hue= topic_column,\n",
    "        palette='tab10'\n",
    "    )\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    plt.xlabel('Month', fontsize=12)\n",
    "    plt.ylabel('Number of Discussions', fontsize=12)\n",
    "    plt.title(titel, fontsize=14)\n",
    "    plt.legend(title='Topic', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "excluded_topics = ['Agriculture', 'Technology', 'Transportation', 'Housing', 'Labor', 'Culture', 'Public']\n",
    "titel = 'Number of Times Each Topic Was Uploaded per Month (excluding the government score)'\n",
    "plot_topic_counts('most_probable_topic_speech_wo_gov', excluded_topics, titel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Populist speech patterns\n",
    "The following code identifies populist speech patterns in the code. First with a BERT derived model and second with a dictionary approach. It therefore serves to answer the hypotheses 2.3 and its sub-hypothesis.\n",
    "\n",
    "First of all the model [PopBERT](https://huggingface.co/luerhard/PopBERT) is used to get an overview. After this the different speech patterns as described and theoretically justified in the thesis are investigated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to MPS device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a suitable dataframe\n",
    "if os.path.exists(\"data/speeches_with_transcriptions_df.pkl\"):\n",
    "    speeches_with_transcriptions_df = pd.read_csv(\"data/speeches_with_transcriptions_df.pkl\")\n",
    "elif os.path.exists(\"data/speeches_with_transcriptions_df.csv\"):\n",
    "    speeches_with_transcriptions_df = pd.read_csv(\"data/speeches_with_transcriptions_df.csv\")\n",
    "else:\n",
    "    print(\"No data found. Please run the alignement code block first.\")\n",
    "\n",
    "swt_scores_df = speeches_with_transcriptions_df.copy()\n",
    "swt_scores_df['sentence_list_speech'] = swt_scores_df['doc'].apply(lambda x: list(nlp(x).sents))\n",
    "swt_scores_df['sentence_list_upload'] = swt_scores_df['transcription_text'].dropna().apply(lambda x: list(nlp(x).sents))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"luerhard/PopBERT\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"luerhard/PopBERT\")\n",
    "\n",
    "# If MPS is available, use it for better performance\n",
    "if torch.backends.mps.is_available():\n",
    "    model = model.to(mps_device)\n",
    "    print(\"Model moved to MPS device\")\n",
    "    \n",
    "    def process_sentence(sentence):\n",
    "        inputs = tokenizer(sentence.text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        inputs = {k: v.to(mps_device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        probs = torch.nn.functional.sigmoid(logits)\n",
    "        return probs.detach().cpu().numpy().tolist()\n",
    "    \n",
    "    swt_scores_df['pop_scores_speech'] = swt_scores_df['sentence_list_speech'].dropna().apply(\n",
    "        lambda x: [process_sentence(sents) for sents in x]\n",
    "    )\n",
    "    swt_scores_df['pop_scores_uploads'] = swt_scores_df['sentence_list_upload'].dropna().apply(\n",
    "        lambda x: [process_sentence(sents) for sents in x]\n",
    "    )\n",
    "else:\n",
    "    def process_sentence(sentence):\n",
    "        inputs = tokenizer(sentence.text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        probs = torch.nn.functional.sigmoid(logits)\n",
    "        return probs.detach().numpy().tolist()\n",
    "    \n",
    "    swt_scores_df['pop_scores_speech'] = swt_scores_df['sentence_list_speech'].dropna().apply(\n",
    "        lambda x: [process_sentence(sents) for sents in x]\n",
    "    )\n",
    "    swt_scores_df['pop_scores_uploads'] = swt_scores_df['sentence_list_upload'].dropna().apply(\n",
    "        lambda x: [process_sentence(sents) for sents in x]\n",
    "    )\n",
    "\n",
    "swt_scores_df.to_csv(\"data/pop_scores.csv\")\n",
    "swt_scores_df.to_pickle(\"data/pop_scores.pkl\")\n",
    "del swt_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if os.path.exists(\"data/pop_scores.pkl\"):\n",
    "    pop_scores_df = pd.read_pickle(\"data/pop_scores.pkl\")\n",
    "elif os.path.exists(\"data/pop_scores.csv\"):\n",
    "    pop_scores_df = pd.read_csv(\"data/pop_scores.csv\")\n",
    "else: \n",
    "    print(\"No data found. Please run the alignement code block first.\")\n",
    "\n",
    "# Compare the scores of the speeches with the thresholds provided by the publisher of the model and classify the texts binaryly\n",
    "thresholds = [[0.415961, 0.295400, 0.429109, 0.302714]]\n",
    "\n",
    "# Safely parse the string representations of the scores\n",
    "def safe_eval(x):\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(x)\n",
    "            # Flatten the nested list if necessary\n",
    "            if isinstance(parsed, list) and all(isinstance(inner, list) for inner in parsed):\n",
    "                return [item for sublist in parsed for item in sublist]\n",
    "            elif isinstance(parsed, list):\n",
    "                return parsed\n",
    "            return [parsed]  # Ensure the result is always a list\n",
    "        except (SyntaxError, ValueError):\n",
    "            return None\n",
    "    return x\n",
    "\n",
    "# Apply the safe evaluation to the columns\n",
    "pop_scores_df['eval_pop_scores_speech'] = pop_scores_df['pop_scores_speech'].dropna().apply(safe_eval)\n",
    "pop_scores_df['eval_pop_scores_uploads'] = pop_scores_df['upload_scores'].dropna().apply(safe_eval)\n",
    "\n",
    "# Evaluate the scores with the thresholds\n",
    "def evaluate_scores(scores, thresholds):\n",
    "    if isinstance(scores, list):\n",
    "        # Flatten the list if it contains nested lists\n",
    "        flat_scores = [item for sublist in scores for item in sublist] if any(isinstance(sublist, list) for sublist in scores) else scores\n",
    "        return [1 if score > thresholds[0][i] else 0 for i, score in enumerate(flat_scores) if i < len(thresholds[0])]\n",
    "    return None\n",
    "\n",
    "# Evaluate the scores with the thresholds elementwise for all tensors in the list\n",
    "def evaluate_tensor_scores(tensor_scores, thresholds):\n",
    "    if isinstance(tensor_scores, list):\n",
    "        return [\n",
    "            [1 if score > thresholds[0][i] else 0 for i, score in enumerate(tensor)]\n",
    "            for tensor in tensor_scores\n",
    "        ]\n",
    "    return None\n",
    "# Apply the evaluation to the columns -> 1 for over threshold, 0 for under threshold\n",
    "pop_scores_df['eval_pop_scores_speech'] = pop_scores_df['eval_pop_scores_speech'].apply(\n",
    "    lambda x: evaluate_tensor_scores(x, thresholds)\n",
    ")\n",
    "pop_scores_df['eval_pop_scores_uploads'] = pop_scores_df['eval_pop_scores_uploads'].apply(\n",
    "    lambda x: evaluate_tensor_scores(x, thresholds)\n",
    ")\n",
    "\n",
    "# Count the number of speeches and uploads that are over the threshold for each category\n",
    "pop_scores_df['speech_number_anti-elitism'] = pop_scores_df['eval_pop_scores_speech'].apply(\n",
    "    lambda x: sum([1 for score in x if score[0] == 1]) if isinstance(x, list) else 0\n",
    ")\n",
    "pop_scores_df['upload_number_anti-elitism'] = pop_scores_df['eval_pop_scores_uploads'].apply(\n",
    "    lambda x: sum([1 for score in x if score[0] == 1]) if isinstance(x, list) else 0\n",
    ")\n",
    "pop_scores_df['speech_number_people-centrism'] = pop_scores_df['eval_pop_scores_speech'].apply(\n",
    "    lambda x: sum([1 for score in x if score[1] == 1]) if isinstance(x, list) else 0\n",
    ")\n",
    "pop_scores_df['upload_number_people-centrism'] = pop_scores_df['eval_pop_scores_uploads'].apply(\n",
    "    lambda x: sum([1 for score in x if score[1] == 1]) if isinstance(x, list) else 0\n",
    ")\n",
    "pop_scores_df['speech_number_lwi'] = pop_scores_df['eval_pop_scores_speech'].apply(\n",
    "    lambda x: sum([1 for score in x if score[2] == 1]) if isinstance(x, list) else 0\n",
    ")\n",
    "pop_scores_df['upload_number_lwi'] = pop_scores_df['eval_pop_scores_uploads'].apply(\n",
    "    lambda x: sum([1 for score in x if score[2] == 1]) if isinstance(x, list) else 0\n",
    ")\n",
    "pop_scores_df['speech_number_rwi'] = pop_scores_df['eval_pop_scores_speech'].apply(\n",
    "    lambda x: sum([1 for score in x if score[3] == 1]) if isinstance(x, list) else 0\n",
    ")\n",
    "pop_scores_df['upload_number_rwi'] = pop_scores_df['eval_pop_scores_uploads'].apply(\n",
    "    lambda x: sum([1 for score in x if score[3] == 1]) if isinstance(x, list) else 0\n",
    ")\n",
    "# Get a length of the speech and upload scores\n",
    "pop_scores_df['speech_scores_length'] = pop_scores_df['eval_pop_scores_speech'].apply(\n",
    "    lambda x: len(x) if isinstance(x, list) else 0\n",
    ")\n",
    "pop_scores_df['upload_scores_length'] = pop_scores_df['eval_pop_scores_uploads'].apply(\n",
    "    lambda x: len(x) if isinstance(x, list) else 0\n",
    ")\n",
    "\n",
    "# Ratio of over threshold scores to length\n",
    "pop_scores_df['speech_ratio_anti-elitism'] = pop_scores_df['speech_number_anti-elitism'] / pop_scores_df['speech_scores_length']\n",
    "pop_scores_df['upload_ratio_anti-elitism'] = pop_scores_df['upload_number_anti-elitism'] / pop_scores_df['upload_scores_length']\n",
    "pop_scores_df['speech_ratio_people-centrism'] = pop_scores_df['speech_number_people-centrism'] / pop_scores_df['speech_scores_length']\n",
    "pop_scores_df['upload_ratio_people-centrism'] = pop_scores_df['upload_number_people-centrism'] / pop_scores_df['upload_scores_length']\n",
    "pop_scores_df['speech_ratio_lwi'] = pop_scores_df['speech_number_lwi'] / pop_scores_df['speech_scores_length']\n",
    "pop_scores_df['upload_ratio_lwi'] = pop_scores_df['upload_number_lwi'] / pop_scores_df['upload_scores_length']\n",
    "pop_scores_df['speech_ratio_rwi'] = pop_scores_df['speech_number_rwi'] / pop_scores_df['speech_scores_length']\n",
    "pop_scores_df['upload_ratio_rwi'] = pop_scores_df['upload_number_rwi'] / pop_scores_df['upload_scores_length']\n",
    "\n",
    "# Results:\n",
    "print(\"Average ratio of anti-elitism in speeches: \", pop_scores_df['speech_ratio_anti-elitism'].mean())\n",
    "print(\"Average ratio of anti-elitism in uploads: \", pop_scores_df['upload_ratio_anti-elitism'].mean())\n",
    "print(\"Average ratio of people-centrism in speeches: \", pop_scores_df['speech_ratio_people-centrism'].mean())\n",
    "print(\"Average ratio of people-centrism in uploads: \", pop_scores_df['upload_ratio_people-centrism'].mean())\n",
    "print(\"Average ratio of lwi in speeches: \", pop_scores_df['speech_ratio_lwi'].mean())\n",
    "print(\"Average ratio of lwi in uploads: \", pop_scores_df['upload_ratio_lwi'].mean())\n",
    "print(\"Average ratio of rwi in speeches: \", pop_scores_df['speech_ratio_rwi'].mean())\n",
    "print(\"Average ratio of rwi in uploads: \", pop_scores_df['upload_ratio_rwi'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ratio of anti-elitism in speeches:  0.15773665441446477\n",
      "Average ratio of anti-elitism in uploads:  0.2878998133606983\n",
      "Average ratio of people-centrism in speeches:  0.022904635131949576\n",
      "Average ratio of people-centrism in uploads:  0.06156495910601404\n",
      "Average ratio of lwi in speeches:  0.012686020924555497\n",
      "Average ratio of lwi in uploads:  0.02081745218503756\n",
      "Average ratio of rwi in speeches:  0.029460141563391537\n",
      "Average ratio of rwi in uploads:  0.07356760760349784\n",
      "Number of speeches with a ratio of anti-elitism over  0.2 :  1239\n",
      "Number of uploads with a ratio of anti-elitism over  0.2 :  236\n",
      "Number of speeches with a ratio of people-centrism over  0.2 :  28\n",
      "Number of uploads with a ratio of people-centrism over  0.2 :  32\n",
      "Number of speeches with a ratio of lwi over  0.2 :  7\n",
      "Number of uploads with a ratio of lwi over  0.2 :  8\n",
      "Number of speeches with a ratio of rwi over  0.2 :  63\n",
      "Number of uploads with a ratio of rwi over  0.2 :  43\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"Average ratio of anti-elitism in speeches: \", pop_scores_df['speech_ratio_anti-elitism'].mean())\n",
    "print(\"Average ratio of anti-elitism in uploads: \", pop_scores_df['upload_ratio_anti-elitism'].mean())\n",
    "print(\"Average ratio of people-centrism in speeches: \", pop_scores_df['speech_ratio_people-centrism'].mean())\n",
    "print(\"Average ratio of people-centrism in uploads: \", pop_scores_df['upload_ratio_people-centrism'].mean())\n",
    "print(\"Average ratio of lwi in speeches: \", pop_scores_df['speech_ratio_lwi'].mean())\n",
    "print(\"Average ratio of lwi in uploads: \", pop_scores_df['upload_ratio_lwi'].mean())\n",
    "print(\"Average ratio of rwi in speeches: \", pop_scores_df['speech_ratio_rwi'].mean())\n",
    "print(\"Average ratio of rwi in uploads: \", pop_scores_df['upload_ratio_rwi'].mean())\n",
    "\n",
    "\n",
    "ratio_treshold = 0.2\n",
    "print(\"Number of speeches with a ratio of anti-elitism over \", ratio_treshold, \": \", pop_scores_df[pop_scores_df['speech_ratio_anti-elitism'] > ratio_treshold].shape[0])\n",
    "print(\"Number of uploads with a ratio of anti-elitism over \", ratio_treshold, \": \", pop_scores_df[pop_scores_df['upload_ratio_anti-elitism'] > ratio_treshold].shape[0])\n",
    "print(\"Number of speeches with a ratio of people-centrism over \", ratio_treshold, \": \", pop_scores_df[pop_scores_df['speech_ratio_people-centrism'] > ratio_treshold].shape[0])\n",
    "print(\"Number of uploads with a ratio of people-centrism over \", ratio_treshold, \": \", pop_scores_df[pop_scores_df['upload_ratio_people-centrism'] > ratio_treshold].shape[0])\n",
    "print(\"Number of speeches with a ratio of lwi over \", ratio_treshold, \": \", pop_scores_df[pop_scores_df['speech_ratio_lwi'] > ratio_treshold].shape[0])\n",
    "print(\"Number of uploads with a ratio of lwi over \", ratio_treshold, \": \", pop_scores_df[pop_scores_df['upload_ratio_lwi'] > ratio_treshold].shape[0])\n",
    "print(\"Number of speeches with a ratio of rwi over \", ratio_treshold, \": \", pop_scores_df[pop_scores_df['speech_ratio_rwi'] > ratio_treshold].shape[0])\n",
    "print(\"Number of uploads with a ratio of rwi over \", ratio_treshold, \": \", pop_scores_df[pop_scores_df['upload_ratio_rwi'] > ratio_treshold].shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dictionary based approches!\n",
    "\n",
    "The dictionaries are provided in this program.\n",
    "\n",
    "- H2.3.1 \"The people\" play a big role\n",
    "- H2.3.2 \"Us vs. them\" arguments play a big role\n",
    "- H2.3.3 Crisis rhetoric plays a big role\n",
    "- H2.3.4 \"Common sense\" arguments play a big role\n",
    "- H2.3.5 Blaming others plays a big role\n",
    "- H2.3.6 The uploaded speeches contain more direct addresses to the governement / other parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Import the df needed for the analysis\n",
    "if os.path.exists(\"data/speeches_with_transcriptions_df.pkl\"):\n",
    "    speeches_with_transcriptions_df = pd.read_pickle(\"data/speeches_with_transcriptions_df.pkl\")\n",
    "elif os.path.exists(\"data/speeches_with_transcriptions_df.csv\"):\n",
    "    speeches_with_transcriptions_df = pd.read_csv(\"data/speeches_with_transcriptions_df.csv\")\n",
    "else:\n",
    "    print(\"No data found. Please run the alignement code block first.\")\n",
    "\n",
    "dict_speeches_with_transcriptions = speeches_with_transcriptions_df.copy()\n",
    "dict_speeches_with_transcriptions['transcription_text_doc'] = dict_speeches_with_transcriptions['transcription_text'].dropna().apply(lambda x: nlp(x))\n",
    "\n",
    "# Import the dictionaries for the quantitative content analysis\n",
    "# The dictionaries are in the form of txt files, where each line is a word or phrase, they are lemmatized\n",
    "peoplecentrism_dict = []\n",
    "with open(\"dictionaries/peoplecentrism.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        peoplecentrism_dict.append(line.strip())\n",
    "\n",
    "usvsthem_dict = []\n",
    "with open(\"dictionaries/usvsthem.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        usvsthem_dict.append(line.strip())\n",
    "\n",
    "crisis_rhetorics_dict = []\n",
    "with open(\"dictionaries/crisis_rhetorics.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        crisis_rhetorics_dict.append(line.strip())\n",
    "\n",
    "common_sense_dict = []\n",
    "with open(\"dictionaries/common_sense.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        common_sense_dict.append(line.strip())\n",
    "\n",
    "blaming_others_dict = []\n",
    "with open(\"dictionaries/blaming_others.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        blaming_others_dict.append(line.strip())\n",
    "\n",
    "contra_gov_dict = []\n",
    "with open(\"dictionaries/contra_gov.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        contra_gov_dict.append(line.strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method counts the number of times a word or phrase from a dictionary appears in a text\n",
    "@Input:\n",
    "    dataframe: the dataframe with the texts to analyze\n",
    "    doc_column: the column in the given df with the spacy doc object of the text\n",
    "    dictionary: the dictionary to use for the analysis\n",
    "@Changes on the dataframe:\n",
    "    Adds a column with the number of times a word or phrase from the dictionary appears in the text\n",
    "    Adds a column with the percentage of words or phrases from the dictionary in the text\n",
    "@Output:\n",
    "    A dataframe with the number of times and percentages a word or phrase from the dictionary appears in the text\n",
    "    A dictionary with the number of times each word or phrase from the dictionary appears in the text\n",
    "\"\"\"\n",
    "def dictionary_analysis(dataframe, dictionary, doc_column, name_of_dict):\n",
    "    # Initialize a dictionary to store term counts\n",
    "    term_counts = {term: 0 for term in dictionary}\n",
    "    \n",
    "    # Count occurrences of dictionary words/phrases in each text\n",
    "    dataframe['dict_count_' + doc_column + name_of_dict] = dataframe[doc_column].dropna().apply(\n",
    "        lambda doc: sum(1 for token in doc if token.lemma_ in dictionary)\n",
    "    )\n",
    "    \n",
    "    # Calculate the percentage of dictionary words/phrases in each text\n",
    "    dataframe['dict_percentage_' + doc_column + name_of_dict] = dataframe[doc_column].dropna().apply(\n",
    "        lambda doc: sum(1 for token in doc if token.lemma_ in dictionary) / len(doc) if len(doc) > 0 else 0\n",
    "    )\n",
    "    \n",
    "    # Update term_counts with the number of occurrences for each term\n",
    "    for doc in dataframe[doc_column].dropna():\n",
    "        for token in doc:\n",
    "            if token.lemma_ in term_counts:\n",
    "                term_counts[token.lemma_] += 1\n",
    "    \n",
    "    return dataframe, term_counts\n",
    "\n",
    "# Apply the dictionary analysis to the dataframe\n",
    "dict_speeches_with_transcriptions, peoplecentrism_occourances_speeches = dictionary_analysis(dict_speeches_with_transcriptions, peoplecentrism_dict, 'doc', 'peoplecentrism')\n",
    "dict_speeches_with_transcriptions, peoplecentrism_occourances_transcriptions = dictionary_analysis(dict_speeches_with_transcriptions, peoplecentrism_dict, 'transcription_text_doc', 'peoplecentrism')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r8/9skhg1fj23156y4c7f_cvm880000gn/T/ipykernel_89050/3074769623.py:22: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=df, x='Type', y='Average Percentage', palette='pastel')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the difference between the speeches and the uploads regaring the occourances of the words in the dictionary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_dictionary_analysis(dataframe, dict_column_speeches, dict_column_uploads, title = 'Average Percentage of Dictionary Words in Speeches vs Uploads'):\n",
    "    # Calculate the average percentage of dictionary words in speeches and uploads\n",
    "    average_percentage_speeches = dataframe[dict_column_speeches].mean()\n",
    "    average_percentage_uploads = dataframe[dict_column_uploads].mean()\n",
    "\n",
    "    # Prepare data for visualization\n",
    "    data = {\n",
    "        'Type': ['Speeches', 'Uploads'],\n",
    "        'Average Percentage': [average_percentage_speeches, average_percentage_uploads]\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame for the plot\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Plot the bar chart\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(data=df, x='Type', y='Average Percentage', palette='pastel')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('Type', fontsize=12)\n",
    "    plt.ylabel('Average Percentage', fontsize=12)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "visualize_dictionary_analysis(\n",
    "    dict_speeches_with_transcriptions, \n",
    "    'dict_percentage_docpeoplecentrism', \n",
    "    'dict_percentage_transcription_text_docpeoplecentrism',\n",
    "    title = 'Average Percentage of People-Centrism Words in Speeches vs Uploads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Emotions\n",
    "\n",
    "The following analysis test the hypotheses\n",
    "\n",
    "- <b> 3.1 </b> The uploaded speeches have a more negative sentiment on average.\n",
    "- <b> 3.2 </b> The uploaded speeches contain more hate speech.\n",
    "- <b> 3.3 </b> The uploaded speeches contain more direct addresses to the citizens / the people.\n",
    "- <b> 3.4 </b> The uploaded speeches contain more direct adrresses to the government / the other parties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Syntax\n",
    "\n",
    "The following analysis test the hypotheses:\n",
    "\n",
    "- <b> 4.1 </b> The uploaded speeches are less complex.\n",
    "    - <b> 4.1.1 </b> They use simpler sentence structures.\n",
    "    - <b> 4.1.2 </b> They use fewer borrowed words.\n",
    "- <b> 4.2 </b> More active sentence structures are used in the uploaded speeches.\n",
    "- <b> 4.3 </b> There are more imperatives and Calls to action in the uploaded speeches.\n",
    "- <b> 4.4 </b> There are more rheorical questions in the uploaded speeches\n",
    "- <b> 4.5 </b> The uploaded speeches contain more repetitions in the speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma_te",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
